{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing  \n",
    "Sofia, Angel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" file = S1_files[0]\\n\\n\\ndf = pd.read_csv(file, header=None)\\ndf.columns = [\\n    'time',\\n    'frontal accel',\\n    'vertical accel',\\n    'lateral accel',\\n    'antenna id',\\n    'rssi',\\n    'phase',\\n    'frequency',\\n    'activity',\\n]\\nactivities = {\\n    1: 'sit on bed',\\n    2: 'sit on chair',\\n    3: 'lying',\\n    4: 'ambulating',\\n}\\ndf.replace({'activity': activities}, inplace=True) \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# list of file name strings from S1 and S2\n",
    "S1_files = sorted(glob.glob('../S1_Dataset/d1p*'))\n",
    "S2_files = sorted(glob.glob('../S2_Dataset/d2p*'))\n",
    " \"\"\"\n",
    "# TODO: when done with program, merge all data from S1 and S2 into 1 giant dataset\n",
    "# for testing only using 1 file for now\n",
    "\"\"\" file = S1_files[0]\n",
    "\n",
    "\n",
    "df = pd.read_csv(file, header=None)\n",
    "df.columns = [\n",
    "    'time',\n",
    "    'frontal accel',\n",
    "    'vertical accel',\n",
    "    'lateral accel',\n",
    "    'antenna id',\n",
    "    'rssi',\n",
    "    'phase',\n",
    "    'frequency',\n",
    "    'activity',\n",
    "]\n",
    "activities = {\n",
    "    1: 'sit on bed',\n",
    "    2: 'sit on chair',\n",
    "    3: 'lying',\n",
    "    4: 'ambulating',\n",
    "}\n",
    "df.replace({'activity': activities}, inplace=True) \"\"\"\n",
    "\n",
    "# TODO: add 3 columns activity status, frequency of activity type (time over total time), and gender \n",
    "# our target is \"activity status\" (inactive or active) with our independent variables being activity type, time, rssi, frontal accel, vertical accel, lateral accel, gender, frequency (of activity type)\n",
    "\n",
    "\n",
    "# TODO: preprocess data and split into test and training sets using df\n",
    "# normalize data \n",
    "# split 25:75 training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above wasn't working when I ran it on my computer. I'm not as experienced in jupyter notebook so maybe I'm doing something wrong? This version of the code should work if you're trying to extract the files from the zipped folder. -- Angel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile, ZipInfo\n",
    "import pandas as pd\n",
    "\n",
    "## this is the zipped folder name on my computer, idk if its the same for everyone\n",
    "zipped_name = 'activity+recognition+with+healthy+older+people+using+a+batteryless+wearable+sensor.zip'\n",
    "\n",
    "## honestly, idk what this is doing, but i feel its important stuff\n",
    "z = ZipFile(zipped_name, 'r')\n",
    "ls1 = z.infolist()\n",
    "ls2 = [file for file in ls1 if file.file_size > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## im going to seperate the files into four categories to make adding extra attributes easier\n",
    "S1F = []\n",
    "S1M = []\n",
    "S2F = []\n",
    "S2M = []\n",
    "## this one is just to check my work\n",
    "other = [] #ignore\n",
    "\n",
    "## i feel like there was a more efficient way to do this\n",
    "## but this segment of code extracts the content of the files\n",
    " # and converts them into pandas dataframes\n",
    "for i in range(len(ls2)):\n",
    "    temp_zip = z.extract(member=ls2[i])\n",
    "    if ((\"S1_Dataset\" in temp_zip) and (\"F\" in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S1F.append(temp_csv)\n",
    "    elif ((\"S1_Dataset\" in temp_zip) and ('M' in temp_zip) and ('.txt' not in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S1M.append(temp_csv)\n",
    "    elif ((\"S2_Dataset\" in temp_zip) and (\"F\" in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S2F.append(temp_csv)\n",
    "    elif ((\"S2_Dataset\" in temp_zip) and (\"M\" in temp_zip) and ('.txt' not in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S2M.append(temp_csv)\n",
    "    else:\n",
    "        other.append(temp_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all dataframes in one list, seperated into four categories\n",
    " # (based on gender and room)\n",
    "all_datasets = [S1F, S1M, S2F, S2M]\n",
    "\n",
    "features = [\n",
    "    'time',\n",
    "    'frontal accel',\n",
    "    'vertical accel',\n",
    "    'lateral accel',\n",
    "    'antenna id',\n",
    "    'rssi',\n",
    "    'phase',\n",
    "    'frequency',\n",
    "    'activity',\n",
    "]\n",
    "\n",
    "activities = [\n",
    "    'sitting on bed',\n",
    "    'sitting on chair',\n",
    "    'lying',\n",
    "    'ambulating'\n",
    "]\n",
    "\n",
    "## in this segment of code, i rename the headers\n",
    "new_headers = dict(enumerate(features))\n",
    "for subset in all_datasets:\n",
    "    for df in subset:\n",
    "        df.rename(columns = new_headers, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  CAN ONLY RUN ONCE APPARENTLY\n",
    "##\n",
    "\n",
    "## adding F/M and room columns to dataframes F=0 and M=1 \n",
    "for df in S1F:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 0)\n",
    "    df.insert(1, 'room', 1)\n",
    "\n",
    "for df in S1M:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 1)\n",
    "    df.insert(1, 'room', 1)\n",
    "\n",
    "for df in S2F:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 0)\n",
    "    df.insert(1, 'room', 2)\n",
    "\n",
    "for df in S2M:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 1)\n",
    "    df.insert(1, 'room', 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go through each dataframe and add additional columns now. In these next cells, I add the 'consecutiveness' feature. This tells us how many times in a row an individual did an activity. \n",
    "\n",
    "For example, the first observation will have a 1 in this column. If the individual continued to do the same activity in the next observation, that obervation will have a 2 for the 'consecutivness' feature.\n",
    "Otherwise, the count will start over at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to add in the 'consecutiveness' attribute in a dataframe\n",
    "def count_activities(dataframe):\n",
    "    counts = []\n",
    "    curr_count = 0\n",
    "    prev_act = None\n",
    "\n",
    "    for act in df['activity']:\n",
    "        if act == prev_act:\n",
    "            curr_count += 1\n",
    "        else:\n",
    "            curr_count = 1\n",
    "            prev_act = act\n",
    "    \n",
    "        counts.append(curr_count)\n",
    "\n",
    "    dataframe.insert(len(dataframe.columns) - 1, 'consecutiveness', counts)\n",
    "    return dataframe\n",
    "\n",
    "list_of_dataframes = S1F + S1M + S2F + S2M\n",
    "\n",
    "##   ONLY RUN ONCE\n",
    "##\n",
    "\n",
    "## adds the column 'consecutiveness' into each dataframe\n",
    "for df in list_of_dataframes:\n",
    "    count_activities(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code adds the acceleration column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def calc_and_add_accel(dataframe):\n",
    "    acceleration = []\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        fa = dataframe['frontal accel'][i]\n",
    "        la = dataframe['lateral accel'][i]\n",
    "        va = dataframe['vertical accel'][i]\n",
    "\n",
    "        accel_vector = np.sqrt(fa**2 + la**2 + va**2)\n",
    "        acceleration.append(accel_vector)\n",
    "\n",
    "    dataframe['acceleration'] = acceleration\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JUST RUN ONCE\n",
    "\n",
    "for df in list_of_dataframes:\n",
    "    calc_and_add_accel(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added binary activity status in the last column. 0 = non-active, 1 = active\n",
    "\n",
    "\n",
    "I also combined all observations into one large dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_df = pd.concat(list_of_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = large_df.shape[0]\n",
    "\n",
    "bed = (large_df['activity'] == 1).sum() / totals\n",
    "chair = (large_df['activity'] == 2).sum() / totals\n",
    "lying = (large_df['activity'] == 3).sum() / totals \n",
    "amb = (large_df['activity'] == 4).sum() / totals \n",
    "\n",
    "act_dict = {1: bed, 2: chair, 3: lying, 4: amb}\n",
    "\n",
    "large_df['freq'] = large_df['activity'].map(act_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_active = [1, 2, 3]\n",
    "\n",
    "binary_activity = []\n",
    "for i in range(large_df.shape[0]):\n",
    "    if large_df['activity'][i] in non_active:\n",
    "        binary_activity.append(0)\n",
    "    else:\n",
    "        binary_activity.append(1)\n",
    "\n",
    "large_df.insert(len(large_df.columns), 'status', binary_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomralize RSSI [-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>room</th>\n",
       "      <th>frontal accel</th>\n",
       "      <th>vertical accel</th>\n",
       "      <th>lateral accel</th>\n",
       "      <th>antenna id</th>\n",
       "      <th>rssi</th>\n",
       "      <th>phase</th>\n",
       "      <th>frequency</th>\n",
       "      <th>gender</th>\n",
       "      <th>consecutiveness</th>\n",
       "      <th>activity</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>freq</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.537313</td>\n",
       "      <td>5.83680</td>\n",
       "      <td>921.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.880597</td>\n",
       "      <td>4.84120</td>\n",
       "      <td>925.75</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.507463</td>\n",
       "      <td>3.64170</td>\n",
       "      <td>924.25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.567164</td>\n",
       "      <td>1.77790</td>\n",
       "      <td>924.75</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.686567</td>\n",
       "      <td>0.24083</td>\n",
       "      <td>922.75</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75123</th>\n",
       "      <td>532.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57689</td>\n",
       "      <td>0.88191</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.716418</td>\n",
       "      <td>3.76290</td>\n",
       "      <td>922.75</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062391</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75124</th>\n",
       "      <td>532.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57689</td>\n",
       "      <td>0.88191</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.746269</td>\n",
       "      <td>5.60210</td>\n",
       "      <td>924.75</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062391</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75125</th>\n",
       "      <td>533.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.35411</td>\n",
       "      <td>0.96229</td>\n",
       "      <td>0.088944</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.791045</td>\n",
       "      <td>0.98175</td>\n",
       "      <td>923.75</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029226</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75126</th>\n",
       "      <td>533.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0.35411</td>\n",
       "      <td>0.96229</td>\n",
       "      <td>0.088944</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.641791</td>\n",
       "      <td>1.46030</td>\n",
       "      <td>922.25</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029226</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75127</th>\n",
       "      <td>544.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.16650</td>\n",
       "      <td>1.01970</td>\n",
       "      <td>0.100350</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.597015</td>\n",
       "      <td>0.15493</td>\n",
       "      <td>922.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.038066</td>\n",
       "      <td>0.030495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75128 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  room  frontal accel  vertical accel  lateral accel  antenna id  \\\n",
       "0        0.00     1        0.51826         0.89339       0.134560           4   \n",
       "1        0.25     1        0.51826         0.89339       0.134560           3   \n",
       "2        0.75     1        0.51826         0.89339       0.134560           4   \n",
       "3        1.25     1        0.51826         0.89339       0.134560           3   \n",
       "4        1.75     1        0.51826         0.89339       0.134560           4   \n",
       "...       ...   ...            ...             ...            ...         ...   \n",
       "75123  532.00     2        0.57689         0.88191       0.134560           3   \n",
       "75124  532.25     2        0.57689         0.88191       0.134560           2   \n",
       "75125  533.50     2        0.35411         0.96229       0.088944           1   \n",
       "75126  533.75     2        0.35411         0.96229       0.088944           3   \n",
       "75127  544.00     2        0.16650         1.01970       0.100350           1   \n",
       "\n",
       "           rssi    phase  frequency  gender  consecutiveness  activity  \\\n",
       "0     -0.537313  5.83680     921.75       0                1         1   \n",
       "1     -0.880597  4.84120     925.75       0                2         1   \n",
       "2     -0.507463  3.64170     924.25       0                3         1   \n",
       "3     -0.567164  1.77790     924.75       0                4         1   \n",
       "4     -0.686567  0.24083     922.75       0                5         1   \n",
       "...         ...      ...        ...     ...              ...       ...   \n",
       "75123 -0.716418  3.76290     922.75       1                7         1   \n",
       "75124 -0.746269  5.60210     924.75       1                8         1   \n",
       "75125 -0.791045  0.98175     923.75       1                9         1   \n",
       "75126 -0.641791  1.46030     922.25       1               10         1   \n",
       "75127 -0.597015  0.15493     922.75       1                1         4   \n",
       "\n",
       "       acceleration      freq  status  \n",
       "0          1.041559  0.218374       0  \n",
       "1          1.041559  0.218374       0  \n",
       "2          1.041559  0.218374       0  \n",
       "3          1.041559  0.218374       0  \n",
       "4          1.041559  0.218374       0  \n",
       "...             ...       ...     ...  \n",
       "75123      1.062391  0.218374       0  \n",
       "75124      1.062391  0.218374       0  \n",
       "75125      1.029226  0.218374       0  \n",
       "75126      1.029226  0.218374       0  \n",
       "75127      1.038066  0.030495       1  \n",
       "\n",
       "[75128 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,0))\n",
    "large_df['rssi'] = scaler.fit_transform(large_df[['rssi']])\n",
    "\n",
    "large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>room</th>\n",
       "      <th>frontal accel</th>\n",
       "      <th>vertical accel</th>\n",
       "      <th>lateral accel</th>\n",
       "      <th>antenna id</th>\n",
       "      <th>rssi</th>\n",
       "      <th>phase</th>\n",
       "      <th>frequency</th>\n",
       "      <th>gender</th>\n",
       "      <th>consecutiveness</th>\n",
       "      <th>activity</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.537313</td>\n",
       "      <td>5.83680</td>\n",
       "      <td>921.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.880597</td>\n",
       "      <td>4.84120</td>\n",
       "      <td>925.75</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.507463</td>\n",
       "      <td>3.64170</td>\n",
       "      <td>924.25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.567164</td>\n",
       "      <td>1.77790</td>\n",
       "      <td>924.75</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.686567</td>\n",
       "      <td>0.24083</td>\n",
       "      <td>922.75</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75123</th>\n",
       "      <td>532.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57689</td>\n",
       "      <td>0.88191</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.716418</td>\n",
       "      <td>3.76290</td>\n",
       "      <td>922.75</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062391</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75124</th>\n",
       "      <td>532.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57689</td>\n",
       "      <td>0.88191</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.746269</td>\n",
       "      <td>5.60210</td>\n",
       "      <td>924.75</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062391</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75125</th>\n",
       "      <td>533.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.35411</td>\n",
       "      <td>0.96229</td>\n",
       "      <td>0.088944</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.791045</td>\n",
       "      <td>0.98175</td>\n",
       "      <td>923.75</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029226</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75126</th>\n",
       "      <td>533.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0.35411</td>\n",
       "      <td>0.96229</td>\n",
       "      <td>0.088944</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.641791</td>\n",
       "      <td>1.46030</td>\n",
       "      <td>922.25</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029226</td>\n",
       "      <td>0.218374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75127</th>\n",
       "      <td>544.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.16650</td>\n",
       "      <td>1.01970</td>\n",
       "      <td>0.100350</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.597015</td>\n",
       "      <td>0.15493</td>\n",
       "      <td>922.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.038066</td>\n",
       "      <td>0.030495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75128 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  room  frontal accel  vertical accel  lateral accel  antenna id  \\\n",
       "0        0.00     1        0.51826         0.89339       0.134560           4   \n",
       "1        0.25     1        0.51826         0.89339       0.134560           3   \n",
       "2        0.75     1        0.51826         0.89339       0.134560           4   \n",
       "3        1.25     1        0.51826         0.89339       0.134560           3   \n",
       "4        1.75     1        0.51826         0.89339       0.134560           4   \n",
       "...       ...   ...            ...             ...            ...         ...   \n",
       "75123  532.00     2        0.57689         0.88191       0.134560           3   \n",
       "75124  532.25     2        0.57689         0.88191       0.134560           2   \n",
       "75125  533.50     2        0.35411         0.96229       0.088944           1   \n",
       "75126  533.75     2        0.35411         0.96229       0.088944           3   \n",
       "75127  544.00     2        0.16650         1.01970       0.100350           1   \n",
       "\n",
       "           rssi    phase  frequency  gender  consecutiveness  activity  \\\n",
       "0     -0.537313  5.83680     921.75       0                1         1   \n",
       "1     -0.880597  4.84120     925.75       0                2         1   \n",
       "2     -0.507463  3.64170     924.25       0                3         1   \n",
       "3     -0.567164  1.77790     924.75       0                4         1   \n",
       "4     -0.686567  0.24083     922.75       0                5         1   \n",
       "...         ...      ...        ...     ...              ...       ...   \n",
       "75123 -0.716418  3.76290     922.75       1                7         1   \n",
       "75124 -0.746269  5.60210     924.75       1                8         1   \n",
       "75125 -0.791045  0.98175     923.75       1                9         1   \n",
       "75126 -0.641791  1.46030     922.25       1               10         1   \n",
       "75127 -0.597015  0.15493     922.75       1                1         4   \n",
       "\n",
       "       acceleration      freq  \n",
       "0          1.041559  0.218374  \n",
       "1          1.041559  0.218374  \n",
       "2          1.041559  0.218374  \n",
       "3          1.041559  0.218374  \n",
       "4          1.041559  0.218374  \n",
       "...             ...       ...  \n",
       "75123      1.062391  0.218374  \n",
       "75124      1.062391  0.218374  \n",
       "75125      1.029226  0.218374  \n",
       "75126      1.029226  0.218374  \n",
       "75127      1.038066  0.030495  \n",
       "\n",
       "[75128 rows x 14 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for hyperparameter testing sample the data to reduce run time\n",
    "sampled_df = large_df.sample(n=1000, random_state=42)\n",
    "\n",
    "X = large_df.drop(columns=['status'])\n",
    "y = large_df['status']\n",
    "train_data, test_data = train_test_split(X, test_size=0.25, random_state=42)\n",
    "train_label, test_label = train_test_split(y, test_size=0.25, random_state=42)\n",
    "\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building + Testing Model\n",
    "\n",
    "**Hyper param testing**  \n",
    "Martin  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# TODO: add hyperparameters that we will be testing as parameters for this function (Martin)\n",
    "\n",
    "# create a NN using SGD as our optimization function and MSE as the loss function\n",
    "def create_model(activation_function_hidden='relu', \n",
    "                 activation_function_output='sigmoid', \n",
    "                 hidden_units=2, \n",
    "                 hidden_layers=1, \n",
    "                 regularizer = tf.keras.regularizers.l2, \n",
    "                 reg_param = 0.01, \n",
    "                 momentum = 0.01, \n",
    "                 learning_rate = 0.001): \n",
    "    # TODO: implement our nueral network structure using SGD as the optimization technique (Martin)\n",
    "\n",
    "    # REMOVE LATER: the code below is only for hyper parameter tuning. Eventually we want to pick 1 set of hyper parameters to use and use those for our final script.\n",
    "    model = tf.keras.Sequential()\n",
    "    # Input layer and first hidden layer\n",
    "    model.add(tf.keras.layers.Dense(hidden_units, input_dim=train_data.shape[1], activation=activation_function_hidden, \n",
    "                    kernel_regularizer = regularizer(reg_param)))\n",
    "    \n",
    "    # Additional hidden layers if specified\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(tf.keras.layers.Dense(hidden_units, activation=activation_function_hidden, \n",
    "                        kernel_regularizer=regularizer(reg_param)))\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=activation_function_output))\n",
    "    \n",
    "    # compile the model using SGD as the optimizer and MSE as our loss function\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rupym\\anaconda3\\envs\\Python7\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1761/1761 [==============================] - 2s 848us/step - loss: 0.2727 - accuracy: 0.9346 - val_loss: 0.2093 - val_accuracy: 0.9697\n",
      "Epoch 2/150\n",
      "1761/1761 [==============================] - 1s 778us/step - loss: 0.1826 - accuracy: 0.9694 - val_loss: 0.1600 - val_accuracy: 0.9697\n",
      "Epoch 3/150\n",
      "1761/1761 [==============================] - 1s 782us/step - loss: 0.1439 - accuracy: 0.9694 - val_loss: 0.1299 - val_accuracy: 0.9697\n",
      "Epoch 4/150\n",
      "1761/1761 [==============================] - 1s 784us/step - loss: 0.1196 - accuracy: 0.9694 - val_loss: 0.1103 - val_accuracy: 0.9697\n",
      "Epoch 5/150\n",
      "1761/1761 [==============================] - 1s 795us/step - loss: 0.1034 - accuracy: 0.9694 - val_loss: 0.0969 - val_accuracy: 0.9697\n",
      "Epoch 6/150\n",
      "1761/1761 [==============================] - 1s 791us/step - loss: 0.0920 - accuracy: 0.9694 - val_loss: 0.0872 - val_accuracy: 0.9697\n",
      "Epoch 7/150\n",
      "1761/1761 [==============================] - 1s 793us/step - loss: 0.0835 - accuracy: 0.9694 - val_loss: 0.0798 - val_accuracy: 0.9697\n",
      "Epoch 8/150\n",
      "1761/1761 [==============================] - 1s 792us/step - loss: 0.0769 - accuracy: 0.9694 - val_loss: 0.0739 - val_accuracy: 0.9697\n",
      "Epoch 9/150\n",
      "1761/1761 [==============================] - 1s 791us/step - loss: 0.0716 - accuracy: 0.9694 - val_loss: 0.0692 - val_accuracy: 0.9697\n",
      "Epoch 10/150\n",
      "1761/1761 [==============================] - 1s 793us/step - loss: 0.0673 - accuracy: 0.9694 - val_loss: 0.0652 - val_accuracy: 0.9697\n",
      "Epoch 11/150\n",
      "1761/1761 [==============================] - 1s 806us/step - loss: 0.0637 - accuracy: 0.9694 - val_loss: 0.0619 - val_accuracy: 0.9697\n",
      "Epoch 12/150\n",
      "1761/1761 [==============================] - 1s 814us/step - loss: 0.0606 - accuracy: 0.9694 - val_loss: 0.0590 - val_accuracy: 0.9697\n",
      "Epoch 13/150\n",
      "1761/1761 [==============================] - 1s 806us/step - loss: 0.0579 - accuracy: 0.9694 - val_loss: 0.0565 - val_accuracy: 0.9697\n",
      "Epoch 14/150\n",
      "1761/1761 [==============================] - 1s 806us/step - loss: 0.0556 - accuracy: 0.9694 - val_loss: 0.0543 - val_accuracy: 0.9697\n",
      "Epoch 15/150\n",
      "1761/1761 [==============================] - 1s 799us/step - loss: 0.0535 - accuracy: 0.9694 - val_loss: 0.0523 - val_accuracy: 0.9697\n",
      "Epoch 16/150\n",
      "1761/1761 [==============================] - 1s 805us/step - loss: 0.0517 - accuracy: 0.9694 - val_loss: 0.0506 - val_accuracy: 0.9697\n",
      "Epoch 17/150\n",
      "1761/1761 [==============================] - 1s 810us/step - loss: 0.0500 - accuracy: 0.9694 - val_loss: 0.0490 - val_accuracy: 0.9697\n",
      "Epoch 18/150\n",
      "1761/1761 [==============================] - 1s 850us/step - loss: 0.0485 - accuracy: 0.9694 - val_loss: 0.0476 - val_accuracy: 0.9697\n",
      "Epoch 19/150\n",
      "1761/1761 [==============================] - 2s 858us/step - loss: 0.0472 - accuracy: 0.9694 - val_loss: 0.0463 - val_accuracy: 0.9697\n",
      "Epoch 20/150\n",
      "1761/1761 [==============================] - 1s 806us/step - loss: 0.0459 - accuracy: 0.9694 - val_loss: 0.0451 - val_accuracy: 0.9697\n",
      "Epoch 21/150\n",
      "1761/1761 [==============================] - 1s 796us/step - loss: 0.0448 - accuracy: 0.9694 - val_loss: 0.0441 - val_accuracy: 0.9697\n",
      "Epoch 22/150\n",
      "1761/1761 [==============================] - 1s 814us/step - loss: 0.0438 - accuracy: 0.9694 - val_loss: 0.0431 - val_accuracy: 0.9697\n",
      "Epoch 23/150\n",
      "1761/1761 [==============================] - 1s 834us/step - loss: 0.0428 - accuracy: 0.9694 - val_loss: 0.0422 - val_accuracy: 0.9697\n",
      "Epoch 24/150\n",
      "1761/1761 [==============================] - 1s 826us/step - loss: 0.0420 - accuracy: 0.9694 - val_loss: 0.0414 - val_accuracy: 0.9697\n",
      "Epoch 25/150\n",
      "1761/1761 [==============================] - 2s 854us/step - loss: 0.0412 - accuracy: 0.9694 - val_loss: 0.0406 - val_accuracy: 0.9697\n",
      "Epoch 26/150\n",
      "1761/1761 [==============================] - 2s 892us/step - loss: 0.0405 - accuracy: 0.9694 - val_loss: 0.0399 - val_accuracy: 0.9697\n",
      "Epoch 27/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0398 - accuracy: 0.9694 - val_loss: 0.0392 - val_accuracy: 0.9697\n",
      "Epoch 28/150\n",
      "1761/1761 [==============================] - 2s 876us/step - loss: 0.0392 - accuracy: 0.9694 - val_loss: 0.0386 - val_accuracy: 0.9697\n",
      "Epoch 29/150\n",
      "1761/1761 [==============================] - 1s 819us/step - loss: 0.0386 - accuracy: 0.9694 - val_loss: 0.0381 - val_accuracy: 0.9697\n",
      "Epoch 30/150\n",
      "1761/1761 [==============================] - 1s 813us/step - loss: 0.0380 - accuracy: 0.9694 - val_loss: 0.0376 - val_accuracy: 0.9697\n",
      "Epoch 31/150\n",
      "1761/1761 [==============================] - 1s 836us/step - loss: 0.0375 - accuracy: 0.9694 - val_loss: 0.0371 - val_accuracy: 0.9697\n",
      "Epoch 32/150\n",
      "1761/1761 [==============================] - 1s 797us/step - loss: 0.0371 - accuracy: 0.9694 - val_loss: 0.0366 - val_accuracy: 0.9697\n",
      "Epoch 33/150\n",
      "1761/1761 [==============================] - 1s 802us/step - loss: 0.0366 - accuracy: 0.9694 - val_loss: 0.0362 - val_accuracy: 0.9697\n",
      "Epoch 34/150\n",
      "1761/1761 [==============================] - 1s 801us/step - loss: 0.0362 - accuracy: 0.9694 - val_loss: 0.0358 - val_accuracy: 0.9697\n",
      "Epoch 35/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0359 - accuracy: 0.9694 - val_loss: 0.0355 - val_accuracy: 0.9697\n",
      "Epoch 36/150\n",
      "1761/1761 [==============================] - 1s 798us/step - loss: 0.0355 - accuracy: 0.9694 - val_loss: 0.0351 - val_accuracy: 0.9697\n",
      "Epoch 37/150\n",
      "1761/1761 [==============================] - 1s 805us/step - loss: 0.0352 - accuracy: 0.9694 - val_loss: 0.0348 - val_accuracy: 0.9697\n",
      "Epoch 38/150\n",
      "1761/1761 [==============================] - 1s 791us/step - loss: 0.0349 - accuracy: 0.9694 - val_loss: 0.0345 - val_accuracy: 0.9697\n",
      "Epoch 39/150\n",
      "1761/1761 [==============================] - 1s 803us/step - loss: 0.0346 - accuracy: 0.9694 - val_loss: 0.0343 - val_accuracy: 0.9697\n",
      "Epoch 40/150\n",
      "1761/1761 [==============================] - 1s 787us/step - loss: 0.0343 - accuracy: 0.9694 - val_loss: 0.0340 - val_accuracy: 0.9697\n",
      "Epoch 41/150\n",
      "1761/1761 [==============================] - 1s 798us/step - loss: 0.0341 - accuracy: 0.9694 - val_loss: 0.0338 - val_accuracy: 0.9697\n",
      "Epoch 42/150\n",
      "1761/1761 [==============================] - 1s 803us/step - loss: 0.0339 - accuracy: 0.9694 - val_loss: 0.0335 - val_accuracy: 0.9697\n",
      "Epoch 43/150\n",
      "1761/1761 [==============================] - 1s 795us/step - loss: 0.0337 - accuracy: 0.9694 - val_loss: 0.0333 - val_accuracy: 0.9697\n",
      "Epoch 44/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0334 - accuracy: 0.9694 - val_loss: 0.0331 - val_accuracy: 0.9697\n",
      "Epoch 45/150\n",
      "1761/1761 [==============================] - 1s 804us/step - loss: 0.0333 - accuracy: 0.9694 - val_loss: 0.0329 - val_accuracy: 0.9697\n",
      "Epoch 46/150\n",
      "1761/1761 [==============================] - 1s 783us/step - loss: 0.0331 - accuracy: 0.9694 - val_loss: 0.0328 - val_accuracy: 0.9697\n",
      "Epoch 47/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0329 - accuracy: 0.9694 - val_loss: 0.0326 - val_accuracy: 0.9697\n",
      "Epoch 48/150\n",
      "1761/1761 [==============================] - 1s 788us/step - loss: 0.0328 - accuracy: 0.9694 - val_loss: 0.0325 - val_accuracy: 0.9697\n",
      "Epoch 49/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0326 - accuracy: 0.9694 - val_loss: 0.0323 - val_accuracy: 0.9697\n",
      "Epoch 50/150\n",
      "1761/1761 [==============================] - 1s 798us/step - loss: 0.0325 - accuracy: 0.9694 - val_loss: 0.0322 - val_accuracy: 0.9697\n",
      "Epoch 51/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0323 - accuracy: 0.9694 - val_loss: 0.0320 - val_accuracy: 0.9697\n",
      "Epoch 52/150\n",
      "1761/1761 [==============================] - 1s 784us/step - loss: 0.0322 - accuracy: 0.9694 - val_loss: 0.0319 - val_accuracy: 0.9697\n",
      "Epoch 53/150\n",
      "1761/1761 [==============================] - 1s 804us/step - loss: 0.0321 - accuracy: 0.9694 - val_loss: 0.0318 - val_accuracy: 0.9697\n",
      "Epoch 54/150\n",
      "1761/1761 [==============================] - 1s 807us/step - loss: 0.0320 - accuracy: 0.9694 - val_loss: 0.0317 - val_accuracy: 0.9697\n",
      "Epoch 55/150\n",
      "1761/1761 [==============================] - 1s 796us/step - loss: 0.0319 - accuracy: 0.9694 - val_loss: 0.0316 - val_accuracy: 0.9697\n",
      "Epoch 56/150\n",
      "1761/1761 [==============================] - 1s 797us/step - loss: 0.0318 - accuracy: 0.9694 - val_loss: 0.0315 - val_accuracy: 0.9697\n",
      "Epoch 57/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0317 - accuracy: 0.9694 - val_loss: 0.0314 - val_accuracy: 0.9697\n",
      "Epoch 58/150\n",
      "1761/1761 [==============================] - 1s 792us/step - loss: 0.0316 - accuracy: 0.9694 - val_loss: 0.0313 - val_accuracy: 0.9697\n",
      "Epoch 59/150\n",
      "1761/1761 [==============================] - 1s 797us/step - loss: 0.0315 - accuracy: 0.9694 - val_loss: 0.0313 - val_accuracy: 0.9697\n",
      "Epoch 60/150\n",
      "1761/1761 [==============================] - 1s 784us/step - loss: 0.0315 - accuracy: 0.9694 - val_loss: 0.0312 - val_accuracy: 0.9697\n",
      "Epoch 61/150\n",
      "1761/1761 [==============================] - 1s 782us/step - loss: 0.0314 - accuracy: 0.9694 - val_loss: 0.0311 - val_accuracy: 0.9697\n",
      "Epoch 62/150\n",
      "1761/1761 [==============================] - 1s 777us/step - loss: 0.0313 - accuracy: 0.9694 - val_loss: 0.0311 - val_accuracy: 0.9697\n",
      "Epoch 63/150\n",
      "1761/1761 [==============================] - 1s 781us/step - loss: 0.0313 - accuracy: 0.9694 - val_loss: 0.0310 - val_accuracy: 0.9697\n",
      "Epoch 64/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0312 - accuracy: 0.9694 - val_loss: 0.0309 - val_accuracy: 0.9697\n",
      "Epoch 65/150\n",
      "1761/1761 [==============================] - 1s 780us/step - loss: 0.0311 - accuracy: 0.9694 - val_loss: 0.0309 - val_accuracy: 0.9697\n",
      "Epoch 66/150\n",
      "1761/1761 [==============================] - 1s 796us/step - loss: 0.0311 - accuracy: 0.9694 - val_loss: 0.0308 - val_accuracy: 0.9697\n",
      "Epoch 67/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0310 - accuracy: 0.9694 - val_loss: 0.0308 - val_accuracy: 0.9697\n",
      "Epoch 68/150\n",
      "1761/1761 [==============================] - 1s 799us/step - loss: 0.0310 - accuracy: 0.9694 - val_loss: 0.0307 - val_accuracy: 0.9697\n",
      "Epoch 69/150\n",
      "1761/1761 [==============================] - 1s 797us/step - loss: 0.0309 - accuracy: 0.9694 - val_loss: 0.0307 - val_accuracy: 0.9697\n",
      "Epoch 70/150\n",
      "1761/1761 [==============================] - 1s 791us/step - loss: 0.0309 - accuracy: 0.9694 - val_loss: 0.0306 - val_accuracy: 0.9697\n",
      "Epoch 71/150\n",
      "1761/1761 [==============================] - 1s 796us/step - loss: 0.0309 - accuracy: 0.9694 - val_loss: 0.0306 - val_accuracy: 0.9697\n",
      "Epoch 72/150\n",
      "1761/1761 [==============================] - 1s 784us/step - loss: 0.0308 - accuracy: 0.9694 - val_loss: 0.0306 - val_accuracy: 0.9697\n",
      "Epoch 73/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0308 - accuracy: 0.9694 - val_loss: 0.0305 - val_accuracy: 0.9697\n",
      "Epoch 74/150\n",
      "1761/1761 [==============================] - 1s 778us/step - loss: 0.0307 - accuracy: 0.9694 - val_loss: 0.0305 - val_accuracy: 0.9697\n",
      "Epoch 75/150\n",
      "1761/1761 [==============================] - 1s 778us/step - loss: 0.0307 - accuracy: 0.9694 - val_loss: 0.0305 - val_accuracy: 0.9697\n",
      "Epoch 76/150\n",
      "1761/1761 [==============================] - 1s 781us/step - loss: 0.0307 - accuracy: 0.9694 - val_loss: 0.0304 - val_accuracy: 0.9697\n",
      "Epoch 77/150\n",
      "1761/1761 [==============================] - 1s 779us/step - loss: 0.0306 - accuracy: 0.9694 - val_loss: 0.0304 - val_accuracy: 0.9697\n",
      "Epoch 78/150\n",
      "1761/1761 [==============================] - 1s 793us/step - loss: 0.0306 - accuracy: 0.9694 - val_loss: 0.0304 - val_accuracy: 0.9697\n",
      "Epoch 79/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0306 - accuracy: 0.9694 - val_loss: 0.0303 - val_accuracy: 0.9697\n",
      "Epoch 80/150\n",
      "1761/1761 [==============================] - 1s 790us/step - loss: 0.0306 - accuracy: 0.9694 - val_loss: 0.0303 - val_accuracy: 0.9697\n",
      "Epoch 81/150\n",
      "1761/1761 [==============================] - 1s 787us/step - loss: 0.0305 - accuracy: 0.9694 - val_loss: 0.0303 - val_accuracy: 0.9697\n",
      "Epoch 82/150\n",
      "1761/1761 [==============================] - 1s 798us/step - loss: 0.0305 - accuracy: 0.9694 - val_loss: 0.0303 - val_accuracy: 0.9697\n",
      "Epoch 83/150\n",
      "1761/1761 [==============================] - 1s 796us/step - loss: 0.0305 - accuracy: 0.9694 - val_loss: 0.0302 - val_accuracy: 0.9697\n",
      "Epoch 84/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0305 - accuracy: 0.9694 - val_loss: 0.0302 - val_accuracy: 0.9697\n",
      "Epoch 85/150\n",
      "1761/1761 [==============================] - 1s 774us/step - loss: 0.0304 - accuracy: 0.9694 - val_loss: 0.0302 - val_accuracy: 0.9697\n",
      "Epoch 86/150\n",
      "1761/1761 [==============================] - 1s 781us/step - loss: 0.0304 - accuracy: 0.9694 - val_loss: 0.0302 - val_accuracy: 0.9697\n",
      "Epoch 87/150\n",
      "1761/1761 [==============================] - 1s 778us/step - loss: 0.0304 - accuracy: 0.9694 - val_loss: 0.0301 - val_accuracy: 0.9697\n",
      "Epoch 88/150\n",
      "1761/1761 [==============================] - 1s 777us/step - loss: 0.0304 - accuracy: 0.9694 - val_loss: 0.0301 - val_accuracy: 0.9697\n",
      "Epoch 89/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0304 - accuracy: 0.9694 - val_loss: 0.0301 - val_accuracy: 0.9697\n",
      "Epoch 90/150\n",
      "1761/1761 [==============================] - 1s 788us/step - loss: 0.0303 - accuracy: 0.9694 - val_loss: 0.0301 - val_accuracy: 0.9697\n",
      "Epoch 91/150\n",
      "1761/1761 [==============================] - 1s 781us/step - loss: 0.0303 - accuracy: 0.9694 - val_loss: 0.0301 - val_accuracy: 0.9697\n",
      "Epoch 92/150\n",
      "1761/1761 [==============================] - 1s 797us/step - loss: 0.0303 - accuracy: 0.9694 - val_loss: 0.0301 - val_accuracy: 0.9697\n",
      "Epoch 93/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0303 - accuracy: 0.9694 - val_loss: 0.0300 - val_accuracy: 0.9697\n",
      "Epoch 94/150\n",
      "1761/1761 [==============================] - 1s 792us/step - loss: 0.0303 - accuracy: 0.9694 - val_loss: 0.0300 - val_accuracy: 0.9697\n",
      "Epoch 95/150\n",
      "1761/1761 [==============================] - 1s 787us/step - loss: 0.0303 - accuracy: 0.9694 - val_loss: 0.0300 - val_accuracy: 0.9697\n",
      "Epoch 96/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0300 - val_accuracy: 0.9697\n",
      "Epoch 97/150\n",
      "1761/1761 [==============================] - 1s 776us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0300 - val_accuracy: 0.9697\n",
      "Epoch 98/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0300 - val_accuracy: 0.9697\n",
      "Epoch 99/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0300 - val_accuracy: 0.9697\n",
      "Epoch 100/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 101/150\n",
      "1761/1761 [==============================] - 1s 790us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 102/150\n",
      "1761/1761 [==============================] - 1s 787us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 103/150\n",
      "1761/1761 [==============================] - 1s 793us/step - loss: 0.0302 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 104/150\n",
      "1761/1761 [==============================] - 1s 793us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 105/150\n",
      "1761/1761 [==============================] - 1s 795us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 106/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 107/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 108/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 109/150\n",
      "1761/1761 [==============================] - 1s 796us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0299 - val_accuracy: 0.9697\n",
      "Epoch 110/150\n",
      "1761/1761 [==============================] - 1s 791us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 111/150\n",
      "1761/1761 [==============================] - 1s 803us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 112/150\n",
      "1761/1761 [==============================] - 1s 802us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 113/150\n",
      "1761/1761 [==============================] - 1s 788us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 114/150\n",
      "1761/1761 [==============================] - 1s 783us/step - loss: 0.0301 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 115/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 116/150\n",
      "1761/1761 [==============================] - 1s 783us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 117/150\n",
      "1761/1761 [==============================] - 1s 779us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 118/150\n",
      "1761/1761 [==============================] - 1s 770us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 119/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 120/150\n",
      "1761/1761 [==============================] - 1s 790us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 121/150\n",
      "1761/1761 [==============================] - 1s 777us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 122/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 123/150\n",
      "1761/1761 [==============================] - 1s 796us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0298 - val_accuracy: 0.9697\n",
      "Epoch 124/150\n",
      "1761/1761 [==============================] - 1s 799us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 125/150\n",
      "1761/1761 [==============================] - 1s 790us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 126/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 127/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 128/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 129/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 130/150\n",
      "1761/1761 [==============================] - 1s 775us/step - loss: 0.0300 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 131/150\n",
      "1761/1761 [==============================] - 1s 782us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 132/150\n",
      "1761/1761 [==============================] - 1s 779us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 133/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 134/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 135/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 136/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 137/150\n",
      "1761/1761 [==============================] - 1s 794us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 138/150\n",
      "1761/1761 [==============================] - 1s 790us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 139/150\n",
      "1761/1761 [==============================] - 1s 782us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 140/150\n",
      "1761/1761 [==============================] - 1s 792us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 141/150\n",
      "1761/1761 [==============================] - 1s 792us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 142/150\n",
      "1761/1761 [==============================] - 1s 786us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 143/150\n",
      "1761/1761 [==============================] - 1s 787us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0297 - val_accuracy: 0.9697\n",
      "Epoch 144/150\n",
      "1761/1761 [==============================] - 1s 787us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0296 - val_accuracy: 0.9697\n",
      "Epoch 145/150\n",
      "1761/1761 [==============================] - 1s 797us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0296 - val_accuracy: 0.9697\n",
      "Epoch 146/150\n",
      "1761/1761 [==============================] - 1s 798us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0296 - val_accuracy: 0.9697\n",
      "Epoch 147/150\n",
      "1761/1761 [==============================] - 1s 795us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0296 - val_accuracy: 0.9697\n",
      "Epoch 148/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0296 - val_accuracy: 0.9697\n",
      "Epoch 149/150\n",
      "1761/1761 [==============================] - 1s 789us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0296 - val_accuracy: 0.9697\n",
      "Epoch 150/150\n",
      "1761/1761 [==============================] - 1s 785us/step - loss: 0.0299 - accuracy: 0.9694 - val_loss: 0.0296 - val_accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "history = model.fit(train_data, train_label, epochs = 150, batch_size = 32, verbose = 1, validation_data = (test_data, test_label))\n",
    "\n",
    "# TODO: implement model evaluation and print/graph anything needed for visualization\n",
    "\n",
    "# graph loss\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='test_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "[[0.02929814]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# save and example of how to use model for prediction\n",
    "#model.save(\"../elderly_activity_model.h5\")\n",
    "\n",
    "loaded_model = load_model(\"../elderly_activity_model.h5\")\n",
    "\n",
    "input_data = pd.DataFrame({\n",
    "            'time': [544.00],\n",
    "            'room': [2],\n",
    "            'frontal accel': [0.16650],\n",
    "            'vertical accel': [1.0798],\n",
    "            'lateral accel': [0.100350],\n",
    "            'antenna id': [1],\n",
    "            'rssi': [-0.597015],\n",
    "            'phase': [0.15493],\n",
    "            'frequency': [922.75],\n",
    "            'gender': [1],\n",
    "            'consecutiveness': [1],\n",
    "            'activity': [4],\n",
    "            'acceleration': [1.038066],\n",
    "            'freq': [0.030495],\n",
    "                }).values\n",
    "\n",
    "predictions = loaded_model.predict(input_data)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
