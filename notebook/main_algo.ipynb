{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing  \n",
    "Sofia, Angel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" file = S1_files[0]\\n\\n\\ndf = pd.read_csv(file, header=None)\\ndf.columns = [\\n    'time',\\n    'frontal accel',\\n    'vertical accel',\\n    'lateral accel',\\n    'antenna id',\\n    'rssi',\\n    'phase',\\n    'frequency',\\n    'activity',\\n]\\nactivities = {\\n    1: 'sit on bed',\\n    2: 'sit on chair',\\n    3: 'lying',\\n    4: 'ambulating',\\n}\\ndf.replace({'activity': activities}, inplace=True) \""
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# list of file name strings from S1 and S2\n",
    "S1_files = sorted(glob.glob('../S1_Dataset/d1p*'))\n",
    "S2_files = sorted(glob.glob('../S2_Dataset/d2p*'))\n",
    " \"\"\"\n",
    "# TODO: when done with program, merge all data from S1 and S2 into 1 giant dataset\n",
    "# for testing only using 1 file for now\n",
    "\"\"\" file = S1_files[0]\n",
    "\n",
    "\n",
    "df = pd.read_csv(file, header=None)\n",
    "df.columns = [\n",
    "    'time',\n",
    "    'frontal accel',\n",
    "    'vertical accel',\n",
    "    'lateral accel',\n",
    "    'antenna id',\n",
    "    'rssi',\n",
    "    'phase',\n",
    "    'frequency',\n",
    "    'activity',\n",
    "]\n",
    "activities = {\n",
    "    1: 'sit on bed',\n",
    "    2: 'sit on chair',\n",
    "    3: 'lying',\n",
    "    4: 'ambulating',\n",
    "}\n",
    "df.replace({'activity': activities}, inplace=True) \"\"\"\n",
    "\n",
    "# TODO: add 3 columns activity status, frequency of activity type (time over total time), and gender \n",
    "# our target is \"activity status\" (inactive or active) with our independent variables being activity type, time, rssi, frontal accel, vertical accel, lateral accel, gender, frequency (of activity type)\n",
    "\n",
    "\n",
    "# TODO: preprocess data and split into test and training sets using df\n",
    "# normalize data \n",
    "# split 25:75 training and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above wasn't working when I ran it on my computer. I'm not as experienced in jupyter notebook so maybe I'm doing something wrong? This version of the code should work if you're trying to extract the files from the zipped folder. -- Angel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile, ZipInfo\n",
    "import pandas as pd\n",
    "\n",
    "## this is the zipped folder name on my computer, idk if its the same for everyone\n",
    "zipped_name = 'activity+recognition+with+healthy+older+people+using+a+batteryless+wearable+sensor.zip'\n",
    "\n",
    "## honestly, idk what this is doing, but i feel its important stuff\n",
    "z = ZipFile(zipped_name, 'r')\n",
    "ls1 = z.infolist()\n",
    "ls2 = [file for file in ls1 if file.file_size > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "## im going to seperate the files into four categories to make adding extra attributes easier\n",
    "S1F = []\n",
    "S1M = []\n",
    "S2F = []\n",
    "S2M = []\n",
    "## this one is just to check my work\n",
    "other = [] #ignore\n",
    "\n",
    "## i feel like there was a more efficient way to do this\n",
    "## but this segment of code extracts the content of the files\n",
    " # and converts them into pandas dataframes\n",
    "for i in range(len(ls2)):\n",
    "    temp_zip = z.extract(member=ls2[i])\n",
    "    if ((\"S1_Dataset\" in temp_zip) and (\"F\" in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S1F.append(temp_csv)\n",
    "    elif ((\"S1_Dataset\" in temp_zip) and ('M' in temp_zip) and ('.txt' not in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S1M.append(temp_csv)\n",
    "    elif ((\"S2_Dataset\" in temp_zip) and (\"F\" in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S2F.append(temp_csv)\n",
    "    elif ((\"S2_Dataset\" in temp_zip) and (\"M\" in temp_zip) and ('.txt' not in temp_zip)):\n",
    "        temp_csv = pd.read_csv(temp_zip, header=None)\n",
    "        S2M.append(temp_csv)\n",
    "    else:\n",
    "        other.append(temp_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all dataframes in one list, seperated into four categories\n",
    " # (based on gender and room)\n",
    "all_datasets = [S1F, S1M, S2F, S2M]\n",
    "\n",
    "features = [\n",
    "    'time',\n",
    "    'frontal accel',\n",
    "    'vertical accel',\n",
    "    'lateral accel',\n",
    "    'antenna id',\n",
    "    'rssi',\n",
    "    'phase',\n",
    "    'frequency',\n",
    "    'activity',\n",
    "]\n",
    "\n",
    "activities = [\n",
    "    'sitting on bed',\n",
    "    'sitting on chair',\n",
    "    'lying',\n",
    "    'ambulating'\n",
    "]\n",
    "\n",
    "## in this segment of code, i rename the headers\n",
    "new_headers = dict(enumerate(features))\n",
    "for subset in all_datasets:\n",
    "    for df in subset:\n",
    "        df.rename(columns = new_headers, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  CAN ONLY RUN ONCE APPARENTLY\n",
    "##\n",
    "\n",
    "## adding F/M and room columns to dataframes F=0 and M=1 \n",
    "for df in S1F:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 0)\n",
    "    df.insert(1, 'room', 1)\n",
    "\n",
    "for df in S1M:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 1)\n",
    "    df.insert(1, 'room', 1)\n",
    "\n",
    "for df in S2F:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 0)\n",
    "    df.insert(1, 'room', 2)\n",
    "\n",
    "for df in S2M:\n",
    "    df.insert(len(df.columns) - 1, 'gender', 1)\n",
    "    df.insert(1, 'room', 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go through each dataframe and add additional columns now. In these next cells, I add the 'consecutiveness' feature. This tells us how many times in a row an individual did an activity. \n",
    "\n",
    "For example, the first observation will have a 1 in this column. If the individual continued to do the same activity in the next observation, that obervation will have a 2 for the 'consecutivness' feature.\n",
    "Otherwise, the count will start over at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to add in the 'consecutiveness' attribute in a dataframe\n",
    "def count_activities(dataframe):\n",
    "    counts = []\n",
    "    curr_count = 0\n",
    "    prev_act = None\n",
    "\n",
    "    for act in df['activity']:\n",
    "        if act == prev_act:\n",
    "            curr_count += 1\n",
    "        else:\n",
    "            curr_count = 1\n",
    "            prev_act = act\n",
    "    \n",
    "        counts.append(curr_count)\n",
    "\n",
    "    dataframe.insert(len(dataframe.columns) - 1, 'consecutiveness', counts)\n",
    "    return dataframe\n",
    "\n",
    "list_of_dataframes = S1F + S1M + S2F + S2M\n",
    "\n",
    "##   ONLY RUN ONCE\n",
    "##\n",
    "\n",
    "## adds the column 'consecutiveness' into each dataframe\n",
    "for df in list_of_dataframes:\n",
    "    count_activities(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section of code adds the acceleration column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def calc_and_add_accel(dataframe):\n",
    "    acceleration = []\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        fa = dataframe['frontal accel'][i]\n",
    "        la = dataframe['lateral accel'][i]\n",
    "        va = dataframe['vertical accel'][i]\n",
    "\n",
    "        accel_vector = np.sqrt(fa**2 + la**2 + va**2)\n",
    "        acceleration.append(accel_vector)\n",
    "\n",
    "    dataframe['acceleration'] = acceleration\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JUST RUN ONCE\n",
    "\n",
    "for df in list_of_dataframes:\n",
    "    calc_and_add_accel(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added binary activity status in the last column. 0 = non-active, 1 = active\n",
    "\n",
    "\n",
    "I also combined all observations into one large dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_df = pd.concat(list_of_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = large_df.shape[0]\n",
    "\n",
    "bed = (large_df['activity'] == 1).sum() / totals\n",
    "chair = (large_df['activity'] == 2).sum() / totals\n",
    "lying = (large_df['activity'] == 3).sum() / totals \n",
    "amb = (large_df['activity'] == 4).sum() / totals \n",
    "\n",
    "act_dict = {1: bed, 2: chair, 3: lying, 4: amb}\n",
    "\n",
    "large_df['freq'] = large_df['activity'].map(act_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_active = [1, 2, 3]\n",
    "\n",
    "binary_activity = []\n",
    "for i in range(large_df.shape[0]):\n",
    "    if large_df['activity'][i] in non_active:\n",
    "        binary_activity.append(0)\n",
    "    else:\n",
    "        binary_activity.append(1)\n",
    "\n",
    "large_df.insert(len(large_df.columns), 'status', binary_activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nomralize RSSI [-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>room</th>\n",
       "      <th>frontal accel</th>\n",
       "      <th>vertical accel</th>\n",
       "      <th>lateral accel</th>\n",
       "      <th>antenna id</th>\n",
       "      <th>rssi</th>\n",
       "      <th>phase</th>\n",
       "      <th>frequency</th>\n",
       "      <th>gender</th>\n",
       "      <th>consecutiveness</th>\n",
       "      <th>activity</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>freq</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.537313</td>\n",
       "      <td>5.83680</td>\n",
       "      <td>921.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.880597</td>\n",
       "      <td>4.84120</td>\n",
       "      <td>925.75</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.507463</td>\n",
       "      <td>3.64170</td>\n",
       "      <td>924.25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.567164</td>\n",
       "      <td>1.77790</td>\n",
       "      <td>924.75</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51826</td>\n",
       "      <td>0.89339</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.686567</td>\n",
       "      <td>0.24083</td>\n",
       "      <td>922.75</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041559</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75123</th>\n",
       "      <td>532.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57689</td>\n",
       "      <td>0.88191</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.716418</td>\n",
       "      <td>3.76290</td>\n",
       "      <td>922.75</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062391</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75124</th>\n",
       "      <td>532.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.57689</td>\n",
       "      <td>0.88191</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.746269</td>\n",
       "      <td>5.60210</td>\n",
       "      <td>924.75</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.062391</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75125</th>\n",
       "      <td>533.50</td>\n",
       "      <td>2</td>\n",
       "      <td>0.35411</td>\n",
       "      <td>0.96229</td>\n",
       "      <td>0.088944</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.791045</td>\n",
       "      <td>0.98175</td>\n",
       "      <td>923.75</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029226</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75126</th>\n",
       "      <td>533.75</td>\n",
       "      <td>2</td>\n",
       "      <td>0.35411</td>\n",
       "      <td>0.96229</td>\n",
       "      <td>0.088944</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.641791</td>\n",
       "      <td>1.46030</td>\n",
       "      <td>922.25</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029226</td>\n",
       "      <td>0.218374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75127</th>\n",
       "      <td>544.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.16650</td>\n",
       "      <td>1.01970</td>\n",
       "      <td>0.100350</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.597015</td>\n",
       "      <td>0.15493</td>\n",
       "      <td>922.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.038066</td>\n",
       "      <td>0.030495</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75128 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  room  frontal accel  vertical accel  lateral accel  antenna id  \\\n",
       "0        0.00     1        0.51826         0.89339       0.134560           4   \n",
       "1        0.25     1        0.51826         0.89339       0.134560           3   \n",
       "2        0.75     1        0.51826         0.89339       0.134560           4   \n",
       "3        1.25     1        0.51826         0.89339       0.134560           3   \n",
       "4        1.75     1        0.51826         0.89339       0.134560           4   \n",
       "...       ...   ...            ...             ...            ...         ...   \n",
       "75123  532.00     2        0.57689         0.88191       0.134560           3   \n",
       "75124  532.25     2        0.57689         0.88191       0.134560           2   \n",
       "75125  533.50     2        0.35411         0.96229       0.088944           1   \n",
       "75126  533.75     2        0.35411         0.96229       0.088944           3   \n",
       "75127  544.00     2        0.16650         1.01970       0.100350           1   \n",
       "\n",
       "           rssi    phase  frequency  gender  consecutiveness  activity  \\\n",
       "0     -0.537313  5.83680     921.75       0                1         1   \n",
       "1     -0.880597  4.84120     925.75       0                2         1   \n",
       "2     -0.507463  3.64170     924.25       0                3         1   \n",
       "3     -0.567164  1.77790     924.75       0                4         1   \n",
       "4     -0.686567  0.24083     922.75       0                5         1   \n",
       "...         ...      ...        ...     ...              ...       ...   \n",
       "75123 -0.716418  3.76290     922.75       1                7         1   \n",
       "75124 -0.746269  5.60210     924.75       1                8         1   \n",
       "75125 -0.791045  0.98175     923.75       1                9         1   \n",
       "75126 -0.641791  1.46030     922.25       1               10         1   \n",
       "75127 -0.597015  0.15493     922.75       1                1         4   \n",
       "\n",
       "       acceleration      freq  status  \n",
       "0          1.041559  0.218374       0  \n",
       "1          1.041559  0.218374       0  \n",
       "2          1.041559  0.218374       0  \n",
       "3          1.041559  0.218374       0  \n",
       "4          1.041559  0.218374       0  \n",
       "...             ...       ...     ...  \n",
       "75123      1.062391  0.218374       0  \n",
       "75124      1.062391  0.218374       0  \n",
       "75125      1.029226  0.218374       0  \n",
       "75126      1.029226  0.218374       0  \n",
       "75127      1.038066  0.030495       1  \n",
       "\n",
       "[75128 rows x 15 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,0))\n",
    "large_df['rssi'] = scaler.fit_transform(large_df[['rssi']])\n",
    "\n",
    "large_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03049018563873212\n",
      "0.030507933127462465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for hyperparameter testing sample the data to reduce run time\n",
    "sampled_df = large_df.sample(n=1000, random_state=42)\n",
    "\n",
    "X = large_df.drop(columns=['status'])\n",
    "y = large_df['status']\n",
    "train_data, test_data, train_label, test_label = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building + Testing Model\n",
    "\n",
    "**Hyper param testing**  \n",
    "Martin  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# F1 score metric\n",
    "def f1_score_metric(train_data, train_label):\n",
    "    train_label_binary = tf.round(train_label)  # Convert probabilities to binary predictions\n",
    "    return f1_score(train_data, train_label_binary)\n",
    "\n",
    "# create a NN using SGD as our optimization function and MSE as the loss function\n",
    "# using the params we found from grid search\n",
    "def create_model(activation_function_hidden='relu', \n",
    "                 activation_function_output='sigmoid', \n",
    "                 hidden_units=2, \n",
    "                 hidden_layers=1, \n",
    "                 regularizer = tf.keras.regularizers.l2, \n",
    "                 reg_param = 0.01, \n",
    "                 momentum = 0.01, \n",
    "                 learning_rate = 0.001): \n",
    "    # TODO: implement our nueral network structure using SGD as the optimization technique (Martin)\n",
    "\n",
    "    # REMOVE LATER: the code below is only for hyper parameter tuning. Eventually we want to pick 1 set of hyper parameters to use and use those for our final script.\n",
    "    model = tf.keras.Sequential()\n",
    "    # Input layer and first hidden layer\n",
    "    model.add(tf.keras.layers.Dense(hidden_units, input_dim=train_data.shape[1], activation=activation_function_hidden, \n",
    "                    kernel_regularizer = regularizer(reg_param)))\n",
    "    \n",
    "    # Additional hidden layers if specified\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(tf.keras.layers.Dense(hidden_units, activation=activation_function_hidden, \n",
    "                        kernel_regularizer=regularizer(reg_param)))\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation=activation_function_output))\n",
    "    \n",
    "    # compile the model using SGD as the optimizer and MSE as our loss function\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rupym\\anaconda3\\envs\\Python7\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1761/1761 [==============================] - 2s 894us/step - loss: 0.2343 - accuracy: 0.9624 - val_loss: 0.1681 - val_accuracy: 0.9695\n",
      "Epoch 2/150\n",
      "1761/1761 [==============================] - 2s 859us/step - loss: 0.0986 - accuracy: 0.9695 - val_loss: 0.0709 - val_accuracy: 0.9695\n",
      "Epoch 3/150\n",
      "1761/1761 [==============================] - 2s 859us/step - loss: 0.0688 - accuracy: 0.9695 - val_loss: 0.0666 - val_accuracy: 0.9695\n",
      "Epoch 4/150\n",
      "1761/1761 [==============================] - 2s 880us/step - loss: 0.0658 - accuracy: 0.9695 - val_loss: 0.0638 - val_accuracy: 0.9695\n",
      "Epoch 5/150\n",
      "1761/1761 [==============================] - 2s 867us/step - loss: 0.0629 - accuracy: 0.9695 - val_loss: 0.0613 - val_accuracy: 0.9695\n",
      "Epoch 6/150\n",
      "1761/1761 [==============================] - 2s 873us/step - loss: 0.0604 - accuracy: 0.9695 - val_loss: 0.0592 - val_accuracy: 0.9695\n",
      "Epoch 7/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0583 - accuracy: 0.9695 - val_loss: 0.0572 - val_accuracy: 0.9695\n",
      "Epoch 8/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0564 - accuracy: 0.9695 - val_loss: 0.0554 - val_accuracy: 0.9695\n",
      "Epoch 9/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0547 - accuracy: 0.9695 - val_loss: 0.0536 - val_accuracy: 0.9695\n",
      "Epoch 10/150\n",
      "1761/1761 [==============================] - 2s 919us/step - loss: 0.0530 - accuracy: 0.9695 - val_loss: 0.0520 - val_accuracy: 0.9695\n",
      "Epoch 11/150\n",
      "1761/1761 [==============================] - 2s 928us/step - loss: 0.0514 - accuracy: 0.9695 - val_loss: 0.0505 - val_accuracy: 0.9695\n",
      "Epoch 12/150\n",
      "1761/1761 [==============================] - 2s 913us/step - loss: 0.0499 - accuracy: 0.9695 - val_loss: 0.0491 - val_accuracy: 0.9695\n",
      "Epoch 13/150\n",
      "1761/1761 [==============================] - 2s 876us/step - loss: 0.0484 - accuracy: 0.9695 - val_loss: 0.0477 - val_accuracy: 0.9695\n",
      "Epoch 14/150\n",
      "1761/1761 [==============================] - 2s 886us/step - loss: 0.0470 - accuracy: 0.9695 - val_loss: 0.0465 - val_accuracy: 0.9695\n",
      "Epoch 15/150\n",
      "1761/1761 [==============================] - 2s 882us/step - loss: 0.0459 - accuracy: 0.9695 - val_loss: 0.0454 - val_accuracy: 0.9695\n",
      "Epoch 16/150\n",
      "1761/1761 [==============================] - 2s 879us/step - loss: 0.0448 - accuracy: 0.9695 - val_loss: 0.0444 - val_accuracy: 0.9695\n",
      "Epoch 17/150\n",
      "1761/1761 [==============================] - 2s 862us/step - loss: 0.0438 - accuracy: 0.9695 - val_loss: 0.0434 - val_accuracy: 0.9695\n",
      "Epoch 18/150\n",
      "1761/1761 [==============================] - 2s 906us/step - loss: 0.0429 - accuracy: 0.9695 - val_loss: 0.0425 - val_accuracy: 0.9695\n",
      "Epoch 19/150\n",
      "1761/1761 [==============================] - 2s 897us/step - loss: 0.0421 - accuracy: 0.9695 - val_loss: 0.0417 - val_accuracy: 0.9695\n",
      "Epoch 20/150\n",
      "1761/1761 [==============================] - 2s 893us/step - loss: 0.0413 - accuracy: 0.9695 - val_loss: 0.0409 - val_accuracy: 0.9695\n",
      "Epoch 21/150\n",
      "1761/1761 [==============================] - 2s 879us/step - loss: 0.0405 - accuracy: 0.9695 - val_loss: 0.0402 - val_accuracy: 0.9695\n",
      "Epoch 22/150\n",
      "1761/1761 [==============================] - 2s 865us/step - loss: 0.0398 - accuracy: 0.9695 - val_loss: 0.0395 - val_accuracy: 0.9695\n",
      "Epoch 23/150\n",
      "1761/1761 [==============================] - 2s 868us/step - loss: 0.0392 - accuracy: 0.9695 - val_loss: 0.0389 - val_accuracy: 0.9695\n",
      "Epoch 24/150\n",
      "1761/1761 [==============================] - 2s 854us/step - loss: 0.0386 - accuracy: 0.9695 - val_loss: 0.0383 - val_accuracy: 0.9695\n",
      "Epoch 25/150\n",
      "1761/1761 [==============================] - 2s 874us/step - loss: 0.0380 - accuracy: 0.9695 - val_loss: 0.0378 - val_accuracy: 0.9695\n",
      "Epoch 26/150\n",
      "1761/1761 [==============================] - 2s 885us/step - loss: 0.0375 - accuracy: 0.9695 - val_loss: 0.0373 - val_accuracy: 0.9695\n",
      "Epoch 27/150\n",
      "1761/1761 [==============================] - 2s 902us/step - loss: 0.0370 - accuracy: 0.9695 - val_loss: 0.0368 - val_accuracy: 0.9695\n",
      "Epoch 28/150\n",
      "1761/1761 [==============================] - 2s 910us/step - loss: 0.0366 - accuracy: 0.9695 - val_loss: 0.0364 - val_accuracy: 0.9695\n",
      "Epoch 29/150\n",
      "1761/1761 [==============================] - 2s 867us/step - loss: 0.0361 - accuracy: 0.9695 - val_loss: 0.0360 - val_accuracy: 0.9695\n",
      "Epoch 30/150\n",
      "1761/1761 [==============================] - 2s 870us/step - loss: 0.0358 - accuracy: 0.9695 - val_loss: 0.0356 - val_accuracy: 0.9695\n",
      "Epoch 31/150\n",
      "1761/1761 [==============================] - 2s 923us/step - loss: 0.0354 - accuracy: 0.9695 - val_loss: 0.0353 - val_accuracy: 0.9695\n",
      "Epoch 32/150\n",
      "1761/1761 [==============================] - 2s 924us/step - loss: 0.0351 - accuracy: 0.9695 - val_loss: 0.0349 - val_accuracy: 0.9695\n",
      "Epoch 33/150\n",
      "1761/1761 [==============================] - 2s 953us/step - loss: 0.0347 - accuracy: 0.9695 - val_loss: 0.0346 - val_accuracy: 0.9695\n",
      "Epoch 34/150\n",
      "1761/1761 [==============================] - 2s 912us/step - loss: 0.0344 - accuracy: 0.9695 - val_loss: 0.0343 - val_accuracy: 0.9695\n",
      "Epoch 35/150\n",
      "1761/1761 [==============================] - 2s 914us/step - loss: 0.0342 - accuracy: 0.9695 - val_loss: 0.0341 - val_accuracy: 0.9695\n",
      "Epoch 36/150\n",
      "1761/1761 [==============================] - 2s 970us/step - loss: 0.0339 - accuracy: 0.9695 - val_loss: 0.0338 - val_accuracy: 0.9695\n",
      "Epoch 37/150\n",
      "1761/1761 [==============================] - 2s 951us/step - loss: 0.0337 - accuracy: 0.9695 - val_loss: 0.0336 - val_accuracy: 0.9695\n",
      "Epoch 38/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0335 - accuracy: 0.9695 - val_loss: 0.0334 - val_accuracy: 0.9695\n",
      "Epoch 39/150\n",
      "1761/1761 [==============================] - 2s 964us/step - loss: 0.0332 - accuracy: 0.9695 - val_loss: 0.0332 - val_accuracy: 0.9695\n",
      "Epoch 40/150\n",
      "1761/1761 [==============================] - 2s 959us/step - loss: 0.0331 - accuracy: 0.9695 - val_loss: 0.0330 - val_accuracy: 0.9695\n",
      "Epoch 41/150\n",
      "1761/1761 [==============================] - 2s 886us/step - loss: 0.0329 - accuracy: 0.9695 - val_loss: 0.0328 - val_accuracy: 0.9695\n",
      "Epoch 42/150\n",
      "1761/1761 [==============================] - 2s 942us/step - loss: 0.0327 - accuracy: 0.9695 - val_loss: 0.0327 - val_accuracy: 0.9695\n",
      "Epoch 43/150\n",
      "1761/1761 [==============================] - 2s 984us/step - loss: 0.0326 - accuracy: 0.9695 - val_loss: 0.0325 - val_accuracy: 0.9695\n",
      "Epoch 44/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0324 - accuracy: 0.9695 - val_loss: 0.0324 - val_accuracy: 0.9695\n",
      "Epoch 45/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0323 - accuracy: 0.9695 - val_loss: 0.0323 - val_accuracy: 0.9695\n",
      "Epoch 46/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0322 - accuracy: 0.9695 - val_loss: 0.0321 - val_accuracy: 0.9695\n",
      "Epoch 47/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0320 - accuracy: 0.9695 - val_loss: 0.0320 - val_accuracy: 0.9695\n",
      "Epoch 48/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0319 - accuracy: 0.9695 - val_loss: 0.0319 - val_accuracy: 0.9695\n",
      "Epoch 49/150\n",
      "1761/1761 [==============================] - 2s 951us/step - loss: 0.0318 - accuracy: 0.9695 - val_loss: 0.0318 - val_accuracy: 0.9695\n",
      "Epoch 50/150\n",
      "1761/1761 [==============================] - 2s 932us/step - loss: 0.0317 - accuracy: 0.9695 - val_loss: 0.0317 - val_accuracy: 0.9695\n",
      "Epoch 51/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0316 - accuracy: 0.9695 - val_loss: 0.0316 - val_accuracy: 0.9695\n",
      "Epoch 52/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0316 - accuracy: 0.9695 - val_loss: 0.0316 - val_accuracy: 0.9695\n",
      "Epoch 53/150\n",
      "1761/1761 [==============================] - 2s 950us/step - loss: 0.0315 - accuracy: 0.9695 - val_loss: 0.0315 - val_accuracy: 0.9695\n",
      "Epoch 54/150\n",
      "1761/1761 [==============================] - 2s 913us/step - loss: 0.0314 - accuracy: 0.9695 - val_loss: 0.0314 - val_accuracy: 0.9695\n",
      "Epoch 55/150\n",
      "1761/1761 [==============================] - 2s 891us/step - loss: 0.0314 - accuracy: 0.9695 - val_loss: 0.0314 - val_accuracy: 0.9695\n",
      "Epoch 56/150\n",
      "1761/1761 [==============================] - 2s 886us/step - loss: 0.0313 - accuracy: 0.9695 - val_loss: 0.0313 - val_accuracy: 0.9695\n",
      "Epoch 57/150\n",
      "1761/1761 [==============================] - 2s 888us/step - loss: 0.0312 - accuracy: 0.9695 - val_loss: 0.0312 - val_accuracy: 0.9695\n",
      "Epoch 58/150\n",
      "1761/1761 [==============================] - 2s 879us/step - loss: 0.0312 - accuracy: 0.9695 - val_loss: 0.0312 - val_accuracy: 0.9695\n",
      "Epoch 59/150\n",
      "1761/1761 [==============================] - 2s 906us/step - loss: 0.0311 - accuracy: 0.9695 - val_loss: 0.0311 - val_accuracy: 0.9695\n",
      "Epoch 60/150\n",
      "1761/1761 [==============================] - 2s 909us/step - loss: 0.0311 - accuracy: 0.9695 - val_loss: 0.0311 - val_accuracy: 0.9695\n",
      "Epoch 61/150\n",
      "1761/1761 [==============================] - 2s 968us/step - loss: 0.0310 - accuracy: 0.9695 - val_loss: 0.0310 - val_accuracy: 0.9695\n",
      "Epoch 62/150\n",
      "1761/1761 [==============================] - 2s 996us/step - loss: 0.0298 - accuracy: 0.9695 - val_loss: 0.0269 - val_accuracy: 0.9695\n",
      "Epoch 63/150\n",
      "1761/1761 [==============================] - 2s 988us/step - loss: 0.0272 - accuracy: 0.9695 - val_loss: 0.0267 - val_accuracy: 0.9695\n",
      "Epoch 64/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0271 - accuracy: 0.9695 - val_loss: 0.0269 - val_accuracy: 0.9695\n",
      "Epoch 65/150\n",
      "1761/1761 [==============================] - 2s 934us/step - loss: 0.0270 - accuracy: 0.9695 - val_loss: 0.0267 - val_accuracy: 0.9695\n",
      "Epoch 66/150\n",
      "1761/1761 [==============================] - 2s 893us/step - loss: 0.0268 - accuracy: 0.9695 - val_loss: 0.0266 - val_accuracy: 0.9695\n",
      "Epoch 67/150\n",
      "1761/1761 [==============================] - 2s 906us/step - loss: 0.0267 - accuracy: 0.9695 - val_loss: 0.0317 - val_accuracy: 0.9695\n",
      "Epoch 68/150\n",
      "1761/1761 [==============================] - 2s 917us/step - loss: 0.0267 - accuracy: 0.9695 - val_loss: 0.0264 - val_accuracy: 0.9695\n",
      "Epoch 69/150\n",
      "1761/1761 [==============================] - 2s 973us/step - loss: 0.0266 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 70/150\n",
      "1761/1761 [==============================] - 2s 940us/step - loss: 0.0265 - accuracy: 0.9695 - val_loss: 0.0263 - val_accuracy: 0.9695\n",
      "Epoch 71/150\n",
      "1761/1761 [==============================] - 2s 889us/step - loss: 0.0264 - accuracy: 0.9695 - val_loss: 0.0341 - val_accuracy: 0.9695\n",
      "Epoch 72/150\n",
      "1761/1761 [==============================] - 2s 943us/step - loss: 0.0264 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 73/150\n",
      "1761/1761 [==============================] - 2s 965us/step - loss: 0.0263 - accuracy: 0.9695 - val_loss: 0.0267 - val_accuracy: 0.9695\n",
      "Epoch 74/150\n",
      "1761/1761 [==============================] - 2s 907us/step - loss: 0.0263 - accuracy: 0.9695 - val_loss: 0.0259 - val_accuracy: 0.9695\n",
      "Epoch 75/150\n",
      "1761/1761 [==============================] - 2s 902us/step - loss: 0.0263 - accuracy: 0.9695 - val_loss: 0.0316 - val_accuracy: 0.9695\n",
      "Epoch 76/150\n",
      "1761/1761 [==============================] - 2s 902us/step - loss: 0.0264 - accuracy: 0.9695 - val_loss: 0.0268 - val_accuracy: 0.9695\n",
      "Epoch 77/150\n",
      "1761/1761 [==============================] - 2s 919us/step - loss: 0.0263 - accuracy: 0.9695 - val_loss: 0.0302 - val_accuracy: 0.9695\n",
      "Epoch 78/150\n",
      "1761/1761 [==============================] - 2s 869us/step - loss: 0.0263 - accuracy: 0.9695 - val_loss: 0.0255 - val_accuracy: 0.9695\n",
      "Epoch 79/150\n",
      "1761/1761 [==============================] - 2s 887us/step - loss: 0.0263 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 80/150\n",
      "1761/1761 [==============================] - 2s 921us/step - loss: 0.0262 - accuracy: 0.9695 - val_loss: 0.0263 - val_accuracy: 0.9695\n",
      "Epoch 81/150\n",
      "1761/1761 [==============================] - 2s 957us/step - loss: 0.0262 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 82/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0262 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 83/150\n",
      "1761/1761 [==============================] - 2s 909us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 84/150\n",
      "1761/1761 [==============================] - 2s 916us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0258 - val_accuracy: 0.9695\n",
      "Epoch 85/150\n",
      "1761/1761 [==============================] - 2s 921us/step - loss: 0.0262 - accuracy: 0.9695 - val_loss: 0.0255 - val_accuracy: 0.9695\n",
      "Epoch 86/150\n",
      "1761/1761 [==============================] - 2s 888us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 87/150\n",
      "1761/1761 [==============================] - 2s 902us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 88/150\n",
      "1761/1761 [==============================] - 2s 939us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0276 - val_accuracy: 0.9695\n",
      "Epoch 89/150\n",
      "1761/1761 [==============================] - 2s 872us/step - loss: 0.0262 - accuracy: 0.9695 - val_loss: 0.0262 - val_accuracy: 0.9695\n",
      "Epoch 90/150\n",
      "1761/1761 [==============================] - 2s 875us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 91/150\n",
      "1761/1761 [==============================] - 2s 878us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0266 - val_accuracy: 0.9695\n",
      "Epoch 92/150\n",
      "1761/1761 [==============================] - 2s 915us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0266 - val_accuracy: 0.9695\n",
      "Epoch 93/150\n",
      "1761/1761 [==============================] - 2s 943us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0260 - val_accuracy: 0.9695\n",
      "Epoch 94/150\n",
      "1761/1761 [==============================] - 2s 879us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0262 - val_accuracy: 0.9695\n",
      "Epoch 95/150\n",
      "1761/1761 [==============================] - 2s 914us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 96/150\n",
      "1761/1761 [==============================] - 2s 951us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 97/150\n",
      "1761/1761 [==============================] - 2s 951us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0276 - val_accuracy: 0.9695\n",
      "Epoch 98/150\n",
      "1761/1761 [==============================] - 2s 975us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0254 - val_accuracy: 0.9695\n",
      "Epoch 99/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0255 - val_accuracy: 0.9695\n",
      "Epoch 100/150\n",
      "1761/1761 [==============================] - 2s 962us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0259 - val_accuracy: 0.9695\n",
      "Epoch 101/150\n",
      "1761/1761 [==============================] - 2s 961us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 102/150\n",
      "1761/1761 [==============================] - 2s 884us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0262 - val_accuracy: 0.9695\n",
      "Epoch 103/150\n",
      "1761/1761 [==============================] - 2s 908us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0254 - val_accuracy: 0.9695\n",
      "Epoch 104/150\n",
      "1761/1761 [==============================] - 2s 943us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0262 - val_accuracy: 0.9695\n",
      "Epoch 105/150\n",
      "1761/1761 [==============================] - 2s 906us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 106/150\n",
      "1761/1761 [==============================] - 2s 943us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 107/150\n",
      "1761/1761 [==============================] - 2s 964us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0264 - val_accuracy: 0.9695\n",
      "Epoch 108/150\n",
      "1761/1761 [==============================] - 2s 947us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0255 - val_accuracy: 0.9695\n",
      "Epoch 109/150\n",
      "1761/1761 [==============================] - 2s 887us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0254 - val_accuracy: 0.9695\n",
      "Epoch 110/150\n",
      "1761/1761 [==============================] - 2s 856us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0287 - val_accuracy: 0.9695\n",
      "Epoch 111/150\n",
      "1761/1761 [==============================] - 2s 876us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0264 - val_accuracy: 0.9695\n",
      "Epoch 112/150\n",
      "1761/1761 [==============================] - 2s 874us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 113/150\n",
      "1761/1761 [==============================] - 2s 875us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0264 - val_accuracy: 0.9695\n",
      "Epoch 114/150\n",
      "1761/1761 [==============================] - 2s 900us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0258 - val_accuracy: 0.9695\n",
      "Epoch 115/150\n",
      "1761/1761 [==============================] - 2s 953us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0272 - val_accuracy: 0.9695\n",
      "Epoch 116/150\n",
      "1761/1761 [==============================] - 2s 952us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0298 - val_accuracy: 0.9695\n",
      "Epoch 117/150\n",
      "1761/1761 [==============================] - 2s 1ms/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0254 - val_accuracy: 0.9695\n",
      "Epoch 118/150\n",
      "1761/1761 [==============================] - 2s 996us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 119/150\n",
      "1761/1761 [==============================] - 2s 969us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0255 - val_accuracy: 0.9695\n",
      "Epoch 120/150\n",
      "1761/1761 [==============================] - 2s 938us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0266 - val_accuracy: 0.9695\n",
      "Epoch 121/150\n",
      "1761/1761 [==============================] - 2s 872us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 122/150\n",
      "1761/1761 [==============================] - 2s 894us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0265 - val_accuracy: 0.9695\n",
      "Epoch 123/150\n",
      "1761/1761 [==============================] - 2s 895us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 124/150\n",
      "1761/1761 [==============================] - 2s 909us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 125/150\n",
      "1761/1761 [==============================] - 2s 925us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 126/150\n",
      "1761/1761 [==============================] - 2s 918us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0336 - val_accuracy: 0.9695\n",
      "Epoch 127/150\n",
      "1761/1761 [==============================] - 2s 987us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 128/150\n",
      "1761/1761 [==============================] - 2s 987us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0256 - val_accuracy: 0.9695\n",
      "Epoch 129/150\n",
      "1761/1761 [==============================] - 2s 922us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0255 - val_accuracy: 0.9695\n",
      "Epoch 130/150\n",
      "1761/1761 [==============================] - 2s 893us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0269 - val_accuracy: 0.9695\n",
      "Epoch 131/150\n",
      "1761/1761 [==============================] - 2s 909us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0261 - val_accuracy: 0.9695\n",
      "Epoch 132/150\n",
      "1761/1761 [==============================] - 2s 927us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0259 - val_accuracy: 0.9695\n",
      "Epoch 133/150\n",
      "1761/1761 [==============================] - 2s 947us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 134/150\n",
      "1761/1761 [==============================] - 2s 943us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0275 - val_accuracy: 0.9695\n",
      "Epoch 135/150\n",
      "1761/1761 [==============================] - 2s 958us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0264 - val_accuracy: 0.9695\n",
      "Epoch 136/150\n",
      "1761/1761 [==============================] - 2s 958us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0268 - val_accuracy: 0.9695\n",
      "Epoch 137/150\n",
      "1761/1761 [==============================] - 2s 880us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0254 - val_accuracy: 0.9695\n",
      "Epoch 138/150\n",
      "1761/1761 [==============================] - 2s 914us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0254 - val_accuracy: 0.9695\n",
      "Epoch 139/150\n",
      "1761/1761 [==============================] - 2s 939us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 140/150\n",
      "1761/1761 [==============================] - 2s 889us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0259 - val_accuracy: 0.9695\n",
      "Epoch 141/150\n",
      "1761/1761 [==============================] - 2s 909us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 142/150\n",
      "1761/1761 [==============================] - 2s 926us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 143/150\n",
      "1761/1761 [==============================] - 2s 905us/step - loss: 0.0261 - accuracy: 0.9695 - val_loss: 0.0270 - val_accuracy: 0.9695\n",
      "Epoch 144/150\n",
      "1761/1761 [==============================] - 2s 899us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0257 - val_accuracy: 0.9695\n",
      "Epoch 145/150\n",
      "1761/1761 [==============================] - 2s 899us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n",
      "Epoch 146/150\n",
      "1761/1761 [==============================] - 2s 892us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0314 - val_accuracy: 0.9695\n",
      "Epoch 147/150\n",
      "1761/1761 [==============================] - 2s 925us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0255 - val_accuracy: 0.9695\n",
      "Epoch 148/150\n",
      "1761/1761 [==============================] - 2s 914us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0254 - val_accuracy: 0.9695\n",
      "Epoch 149/150\n",
      "1761/1761 [==============================] - 2s 920us/step - loss: 0.0259 - accuracy: 0.9695 - val_loss: 0.0262 - val_accuracy: 0.9695\n",
      "Epoch 150/150\n",
      "1761/1761 [==============================] - 2s 926us/step - loss: 0.0260 - accuracy: 0.9695 - val_loss: 0.0253 - val_accuracy: 0.9695\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABd9UlEQVR4nO3dd3xUVf7G8c+dSWbSG4GE0EIH6dJERNkFpbgoYoNlpaxlUbAsi6v+VEAsKKJiW7Csda24YltAAQFFqjSVJiodQk/vM+f3xyQDkRZImBuS5/1yXsncuXPv90yAPJ5z7rmWMcYgIiIiUoU47C5AREREJNAUgERERKTKUQASERGRKkcBSERERKocBSARERGpchSAREREpMpRABIREZEqRwFIREREqhwFIBEREalyFIBEqphhw4aRnJx8Ru8dP348lmWVb0EVzNatW7EsizfeeMPuUkTkLFIAEqkgLMsq1WPBggV2l1rlJScnl+pnVV4h6rHHHuOTTz4p1b7FAW7y5Mnlcm6RyirI7gJExOftt98u8fytt95izpw5x2xv3rx5mc7zyiuv4PV6z+i9DzzwAPfee2+Zzl8ZTJkyhczMTP/zmTNn8t577/HMM88QHx/v337hhReWy/kee+wxrrnmGvr3718uxxMRBSCRCuMvf/lLiedLly5lzpw5x2z/vezsbMLCwkp9nuDg4DOqDyAoKIigIP2z8fsgkpKSwnvvvUf//v3PeHhRRAJLQ2Ai55Du3bvTsmVLVq5cycUXX0xYWBj/93//B8Cnn37K5ZdfTlJSEm63m4YNG/Lwww/j8XhKHOP3c4COHjJ5+eWXadiwIW63m44dO7JixYoS7z3eHCDLshg1ahSffPIJLVu2xO1206JFC2bPnn1M/QsWLKBDhw6EhITQsGFDXnrppVLPK/r222+59tprqVu3Lm63mzp16vD3v/+dnJycY9oXERHBrl276N+/PxEREVSvXp0xY8Yc81mkpqYybNgwoqOjiYmJYejQoaSmpp6yltL6z3/+Q/v27QkNDSUuLo6BAweyY8eOEvts3ryZq6++msTEREJCQqhduzYDBw4kLS0N8H2+WVlZvPnmm/6htWHDhpW5tn379nHjjTeSkJBASEgIbdq04c033zxmv/fff5/27dsTGRlJVFQUrVq14tlnn/W/XlBQwEMPPUTjxo0JCQmhWrVqXHTRRcyZM6fMNYqcTfpfOZFzzMGDB+nTpw8DBw7kL3/5CwkJCQC88cYbREREMHr0aCIiIvj6668ZO3Ys6enpPPnkk6c87rvvvktGRgZ/+9vfsCyLSZMmMWDAAH777bdT9hotWrSIjz/+mNtuu43IyEiee+45rr76arZv3061atUAWL16Nb1796ZmzZo89NBDeDweJkyYQPXq1UvV7unTp5Odnc2tt95KtWrVWL58Oc8//zw7d+5k+vTpJfb1eDz06tWLzp07M3nyZObOnctTTz1Fw4YNufXWWwEwxnDllVeyaNEiRowYQfPmzZkxYwZDhw4tVT2n8uijj/Lggw9y3XXXcdNNN7F//36ef/55Lr74YlavXk1MTAz5+fn06tWLvLw8br/9dhITE9m1axdffPEFqampREdH8/bbb3PTTTfRqVMnbrnlFgAaNmxYptpycnLo3r07v/zyC6NGjaJ+/fpMnz6dYcOGkZqayp133gnAnDlzGDRoED169OCJJ54AYMOGDXz33Xf+fcaPH8/EiRP9Naanp/P999+zatUqLr300jLVKXJWGRGpkEaOHGl+/1f0kksuMYCZNm3aMftnZ2cfs+1vf/ubCQsLM7m5uf5tQ4cONfXq1fM/37JliwFMtWrVzKFDh/zbP/30UwOYzz//3L9t3Lhxx9QEGJfLZX755Rf/trVr1xrAPP/88/5t/fr1M2FhYWbXrl3+bZs3bzZBQUHHHPN4jte+iRMnGsuyzLZt20q0DzATJkwosW+7du1M+/bt/c8/+eQTA5hJkyb5txUWFppu3boZwLz++uunrKnYk08+aQCzZcsWY4wxW7duNU6n0zz66KMl9vvxxx9NUFCQf/vq1asNYKZPn37S44eHh5uhQ4eWqpbin+eTTz55wn2mTJliAPOf//zHvy0/P9906dLFREREmPT0dGOMMXfeeaeJiooyhYWFJzxWmzZtzOWXX16q2kQqEg2BiZxj3G43w4cPP2Z7aGio//uMjAwOHDhAt27dyM7OZuPGjac87vXXX09sbKz/ebdu3QD47bffTvnenj17luiVaN26NVFRUf73ejwe5s6dS//+/UlKSvLv16hRI/r06XPK40PJ9mVlZXHgwAEuvPBCjDGsXr36mP1HjBhR4nm3bt1KtGXmzJkEBQX5e4QAnE4nt99+e6nqOZmPP/4Yr9fLddddx4EDB/yPxMREGjduzPz58wGIjo4G4MsvvyQ7O7vM5y2tmTNnkpiYyKBBg/zbgoODueOOO8jMzGThwoUAxMTEkJWVddLhrJiYGNatW8fmzZvPet0i5UkBSOQcU6tWLVwu1zHb161bx1VXXUV0dDRRUVFUr17dP4G6eD7JydStW7fE8+IwdPjw4dN+b/H7i9+7b98+cnJyaNSo0TH7HW/b8Wzfvp1hw4YRFxfnn9dzySWXAMe2LyQk5JihtaPrAdi2bRs1a9YkIiKixH5NmzYtVT0ns3nzZowxNG7cmOrVq5d4bNiwgX379gFQv359Ro8ezauvvkp8fDy9evXixRdfLNXPqyy2bdtG48aNcThK/goovsJw27ZtANx22200adKEPn36ULt2bf76178eM7drwoQJpKam0qRJE1q1asXdd9/NDz/8cFbrFykPmgMkco45uiekWGpqKpdccglRUVFMmDCBhg0bEhISwqpVq7jnnntKddm70+k87nZjzFl9b2l4PB4uvfRSDh06xD333EOzZs0IDw9n165dDBs27Jj2naieQPF6vViWxaxZs45by9Gh66mnnmLYsGF8+umnfPXVV9xxxx1MnDiRpUuXUrt27UCWfYwaNWqwZs0avvzyS2bNmsWsWbN4/fXXGTJkiH/C9MUXX8yvv/7qr//VV1/lmWeeYdq0adx000221i9yMgpAIpXAggULOHjwIB9//DEXX3yxf/uWLVtsrOqIGjVqEBISwi+//HLMa8fb9ns//vgjP//8M2+++SZDhgzxby/LlUb16tVj3rx5ZGZmlggkmzZtOuNjFmvYsCHGGOrXr0+TJk1OuX+rVq1o1aoVDzzwAIsXL6Zr165MmzaNRx55BKDcV9+uV68eP/zwA16vt0QvUPFQab169fzbXC4X/fr1o1+/fni9Xm677TZeeuklHnzwQX/vXVxcHMOHD2f48OFkZmZy8cUXM378eAUgqdA0BCZSCRT3Mhzd45Kfn8+//vUvu0oqwel00rNnTz755BN2797t3/7LL78wa9asUr0fSrbPGFPicuzT1bdvXwoLC5k6dap/m8fj4fnnnz/jYxYbMGAATqeThx566JheMGMMBw8eBCA9PZ3CwsISr7dq1QqHw0FeXp5/W3h4eLlent+3b19SUlL44IMP/NsKCwt5/vnniYiI8A8tFtdZzOFw0Lp1awB/fb/fJyIigkaNGpWoX6QiUg+QSCVw4YUXEhsby9ChQ7njjjuwLIu333673IagysP48eP56quv6Nq1K7feeisej4cXXniBli1bsmbNmpO+t1mzZjRs2JAxY8awa9cuoqKi+O9//1uq+Ukn0q9fP7p27cq9997L1q1bOe+88/j444/LZf5Nw4YNeeSRR7jvvvvYunUr/fv3JzIyki1btjBjxgxuueUWxowZw9dff82oUaO49tpradKkCYWFhbz99ts4nU6uvvpq//Hat2/P3Llzefrpp0lKSqJ+/fp07tz5pDXMmzeP3NzcY7b379+fW265hZdeeolhw4axcuVKkpOT+eijj/juu++YMmUKkZGRANx0000cOnSIP/7xj9SuXZtt27bx/PPP07ZtW/98ofPOO4/u3bvTvn174uLi+P777/noo48YNWpUmT9HkbPKpqvPROQUTnQZfIsWLY67/3fffWcuuOACExoaapKSksw///lP8+WXXxrAzJ8/37/fiS6DP95l04AZN26c//mJLoMfOXLkMe+tV6/eMZduz5s3z7Rr1864XC7TsGFD8+qrr5p//OMfJiQk5ASfwhHr1683PXv2NBERESY+Pt7cfPPN/svtj75kfejQoSY8PPyY9x+v9oMHD5obbrjBREVFmejoaHPDDTf4L00vy2Xwxf773/+aiy66yISHh5vw8HDTrFkzM3LkSLNp0yZjjDG//fab+etf/2oaNmxoQkJCTFxcnPnDH/5g5s6dW+I4GzduNBdffLEJDQ01wEkviS/+eZ7o8fbbbxtjjNm7d68ZPny4iY+PNy6Xy7Rq1eqYNn/00UfmsssuMzVq1DAul8vUrVvX/O1vfzN79uzx7/PII4+YTp06mZiYGBMaGmqaNWtmHn30UZOfn1/qz0/EDpYxFeh/EUWkyunfv78uoxaRgNMcIBEJmN/ftmLz5s3MnDmT7t2721OQiFRZ6gESkYCpWbMmw4YNo0GDBmzbto2pU6eSl5fH6tWrady4sd3liUgVoknQIhIwvXv35r333iMlJQW3202XLl147LHHFH5EJODUAyQiIiJVjuYAiYiISJWjACQiIiJVjuYAHYfX62X37t1ERkaW+xL0IiIicnYYY8jIyCApKemYm/3+ngLQcezevZs6derYXYaIiIicgR07dpzyZsIKQMdRvAz8jh07iIqKsrkaERERKY309HTq1Knj/z1+MgpAx1E87BUVFaUAJCIico4pzfQVTYIWERGRKkcBSERERKocBSARERGpcjQHSEREqgyPx0NBQYHdZcgZCg4Oxul0lsuxFIBERKTSM8aQkpJCamqq3aVIGcXExJCYmFjmdfoUgEREpNIrDj81atQgLCxMi9yeg4wxZGdns2/fPgBq1qxZpuMpAImISKXm8Xj84adatWp2lyNlEBoaCsC+ffuoUaNGmYbDNAlaREQqteI5P2FhYTZXIuWh+OdY1rlcCkAiIlIlaNirciivn6MCkIiIiFQ5CkAiIiJVQHJyMlOmTCmXYy1YsADLss7pq+o0CVpERKSC6t69O23bti2X4LJixQrCw8PLXlQloQAUQBm5BaTlFBDmCiIu3GV3OSIico4zxuDxeAgKOvWv8+rVqwegonOHhsAC6M3FW7noifk8MWuj3aWIiEgFN2zYMBYuXMizzz6LZVlYlsUbb7yBZVnMmjWL9u3b43a7WbRoEb/++itXXnklCQkJRERE0LFjR+bOnVvieL8fArMsi1dffZWrrrqKsLAwGjduzGeffXbG9f73v/+lRYsWuN1ukpOTeeqpp0q8/q9//YvGjRsTEhJCQkIC11xzjf+1jz76iFatWhEaGkq1atXo2bMnWVlZZ1xLaagHKICcDl/eLPQamysREanajDHkFHgCft7QYGepr2J69tln+fnnn2nZsiUTJkwAYN26dQDce++9TJ48mQYNGhAbG8uOHTvo27cvjz76KG63m7feeot+/fqxadMm6tate8JzPPTQQ0yaNIknn3yS559/nsGDB7Nt2zbi4uJOq10rV67kuuuuY/z48Vx//fUsXryY2267jWrVqjFs2DC+//577rjjDt5++20uvPBCDh06xLfffgvAnj17GDRoEJMmTeKqq64iIyODb7/9FmPO7u9KBaAACnL4/tB7z/IPVURETi6nwMN5Y78M+HnXT+hFmKt0v3qjo6NxuVyEhYWRmJgIwMaNvhGECRMmcOmll/r3jYuLo02bNv7nDz/8MDNmzOCzzz5j1KhRJzzHsGHDGDRoEACPPfYYzz33HMuXL6d3796n1a6nn36aHj168OCDDwLQpEkT1q9fz5NPPsmwYcPYvn074eHh/OlPfyIyMpJ69erRrl07wBeACgsLGTBgAPXq1QOgVatWp3X+M6EhsAByFAUg9QCJiEhZdOjQocTzzMxMxowZQ/PmzYmJiSEiIoINGzawffv2kx6ndevW/u/Dw8OJiory32ridGzYsIGuXbuW2Na1a1c2b96Mx+Ph0ksvpV69ejRo0IAbbriBd955h+zsbADatGlDjx49aNWqFddeey2vvPIKhw8fPu0aTpd6gALI3wOkACQiYqvQYCfrJ/Sy5bzl4fdXc40ZM4Y5c+YwefJkGjVqRGhoKNdccw35+fknPU5wcHCJ55Zl4fV6y6XGo0VGRrJq1SoWLFjAV199xdixYxk/fjwrVqwgJiaGOXPmsHjxYr766iuef/557r//fpYtW0b9+vXLvZZi6gEKoCM9QOX/h0tERErPsizCXEEBf5zuKsYulwuP59Rzlb777juGDRvGVVddRatWrUhMTGTr1q1n+OmcvubNm/Pdd98dU1OTJk389+sKCgqiZ8+eTJo0iR9++IGtW7fy9ddfA76fR9euXXnooYdYvXo1LpeLGTNmnNWa1QMUQMU9QB7lHxERKYXk5GSWLVvG1q1biYiIOGHvTOPGjfn444/p168flmXx4IMPnpWenBP5xz/+QceOHXn44Ye5/vrrWbJkCS+88AL/+te/APjiiy/47bffuPjii4mNjWXmzJl4vV6aNm3KsmXLmDdvHpdddhk1atRg2bJl7N+/n+bNm5/VmtUDFEBOqzgAKQGJiMipjRkzBqfTyXnnnUf16tVPOKfn6aefJjY2lgsvvJB+/frRq1cvzj///IDVef755/Phhx/y/vvv07JlS8aOHcuECRMYNmwYADExMXz88cf88Y9/pHnz5kybNo333nuPFi1aEBUVxTfffEPfvn1p0qQJDzzwAE899RR9+vQ5qzVb5mxfZ3YOSk9PJzo6mrS0NKKiosrtuP9duZN/TF/LxU2q89ZfO5XbcUVE5MRyc3PZsmUL9evXJyQkxO5ypIxO9vM8nd/f6gEKIKdDPUAiIiIVgQJQAB0JQOp0ExGRimvEiBFEREQc9zFixAi7yysXmgQdQEEKQCIicg6YMGECY8aMOe5r5Tk1xE4KQAGkhRBFRORcUKNGDWrUqGF3GWeVhsACSAshioiIVAwKQAGkHiAREZGKQQEogDQHSEREpGJQAAqgIwshKgCJiIjYSQEogPyXwWvtSREREVspAAWQ1gESEZFzydatW7EsizVr1thdSrlTAAogBSARETkd3bt356677iq34w0bNoz+/fuX2/HOZQpAARTk8H3cCkAiIiL2UgAKoKL8o8vgRUTklIYNG8bChQt59tlnsSwLy7LYunUrP/30E3369CEiIoKEhARuuOEGDhw44H/fRx99RKtWrQgNDaVatWr07NmTrKwsxo8fz5tvvsmnn37qP96CBQtOu66FCxfSqVMn3G43NWvW5N5776WwsPCU5wdYsGABnTp1Ijw8nJiYGLp27cq2bdvK/FmdCa0EHUDFPUBaCFFExGbGQEF24M8bHAZFVwSfyrPPPsvPP/9My5YtmTBhgu/twcF06tSJm266iWeeeYacnBzuuecerrvuOr7++mv27NnDoEGDmDRpEldddRUZGRl8++23GGMYM2YMGzZsID09nddffx2AuLi40yp/165d9O3bl2HDhvHWW2+xceNGbr75ZkJCQhg/fvxJz19YWEj//v25+eabee+998jPz2f58uVYpfw8ypsCUAA51QMkIlIxFGTDY0mBP+//7QZXeKl2jY6OxuVyERYWRmJiIgCPPPII7dq147HHHvPv99prr1GnTh1+/vlnMjMzKSwsZMCAAdSrVw+AVq1a+fcNDQ0lLy/Pf7zT9a9//Ys6derwwgsvYFkWzZo1Y/fu3dxzzz2MHTuWPXv2nPD8hw4dIi0tjT/96U80bNgQgObNm59RHeVBQ2AB5FQPkIiIlMHatWuZP39+ibuzN2vWDIBff/2VNm3a0KNHD1q1asW1117LK6+8wuHDh8vt/Bs2bKBLly4lem26du1KZmYmO3fuPOn54+LiGDZsGL169aJfv348++yz7Nmzp9xqO13qAQqg4oUQ1QMkImKz4DBfb4wd5y2DzMxM+vXrxxNPPHHMazVr1sTpdDJnzhwWL17MV199xfPPP8/999/PsmXLqF+/fpnOXRqnOv/rr7/OHXfcwezZs/nggw944IEHmDNnDhdccMFZr+331AMUQE6nFkIUEakQLMs3FBXox2nOd3G5XHg8Hv/z888/n3Xr1pGcnEyjRo1KPMLDw4uaZtG1a1ceeughVq9ejcvlYsaMGcc93ulq3rw5S5YswRz1e+y7774jMjKS2rVrn/L8AO3ateO+++5j8eLFtGzZknffffeM6ykLBaAA0q0wRETkdCQnJ7Ns2TK2bt3KgQMHGDlyJIcOHWLQoEGsWLGCX3/9lS+//JLhw4fj8XhYtmwZjz32GN9//z3bt2/n448/Zv/+/f65NsnJyfzwww9s2rSJAwcOUFBQcFr13HbbbezYsYPbb7+djRs38umnnzJu3DhGjx6Nw+E46fm3bNnCfffdx5IlS9i2bRtfffUVmzdvtm0ekIbAAujohRCNMbbNfBcRkXPDmDFjGDp0KOeddx45OTls2bKF7777jnvuuYfLLruMvLw86tWrR+/evXE4HERFRfHNN98wZcoU0tPTqVevHk899RR9+vQB4Oabb2bBggV06NCBzMxM5s+fT/fu3UtdT61atZg5cyZ33303bdq0IS4ujhtvvJEHHngA4KTn37t3Lxs3buTNN9/k4MGD1KxZk5EjR/K3v/3tbHx0p2QZo/GY30tPTyc6Opq0tDSioqLK7biHs/Jp9/AcAH59rK8/EImIyNmTm5vLli1bqF+/PiEhIXaXI2V0sp/n6fz+1hBYADmOCjyFXq+NlYiIiFRtCkABFHRUAFL+ERERuz322GMlLqk/+lE8bFZZaQ5QADmP6QFy2leMiIhUeSNGjOC666477muhoaEBriawFIACyKkeIBERqUDi4uJO+3YYlYWGwALIaWkOkIiISEWgABRADoflXwNLiyGKiASWV//jWSmU189RQ2AB5rQsCo3RYogiIgHicrlwOBzs3r2b6tWr43K5tA7bOcgYQ35+Pvv378fhcOByucp0PAWgAHM6LAq9CkAiIoHicDioX78+e/bsYfduG+7/JeUqLCyMunXr4nCUbRBLASjAghwWeeh2GCIigeRyuahbty6FhYVluheW2MvpdBIUFFQuPXgKQAHmcOh+YCIidrAsi+DgYIKDg+0uRSoATYIOsCAFIBEREdspAAVY8VpAhQpAIiIitlEACjCneoBERERspwAUYMWLISoAiYiI2EcBKMCczqIApIUQRUREbKMAFGDqARIREbGfAlCAaQ6QiIiI/RSAAiyoaOVKBSARERH7KAAFmBZCFBERsZ8CUIBpIUQRERH7KQAFmEMLIYqIiNhOASjA1AMkIiJivwoRgF588UWSk5MJCQmhc+fOLF++/IT7vvLKK3Tr1o3Y2FhiY2Pp2bPnMfsbYxg7diw1a9YkNDSUnj17snnz5rPdjFLRZfAiIiL2sz0AffDBB4wePZpx48axatUq2rRpQ69evdi3b99x91+wYAGDBg1i/vz5LFmyhDp16nDZZZexa9cu/z6TJk3iueeeY9q0aSxbtozw8HB69epFbm5uoJp1Qv7L4LUQooiIiG0sY+z9Tdy5c2c6duzICy+8AIDX66VOnTrcfvvt3Hvvvad8v8fjITY2lhdeeIEhQ4ZgjCEpKYl//OMfjBkzBoC0tDQSEhJ44403GDhw4CmPmZ6eTnR0NGlpaURFRZWtgb/zl1eXseiXAzxzfRuuale7XI8tIiJSlZ3O729be4Dy8/NZuXIlPXv29G9zOBz07NmTJUuWlOoY2dnZFBQUEBcXB8CWLVtISUkpcczo6Gg6d+58wmPm5eWRnp5e4nG2HFkI8aydQkRERE7B1gB04MABPB4PCQkJJbYnJCSQkpJSqmPcc889JCUl+QNP8ftO55gTJ04kOjra/6hTp87pNqXUjgQgJSARERG72D4HqCwef/xx3n//fWbMmEFISMgZH+e+++4jLS3N/9ixY0c5VlmSeoBERETsF2TnyePj43E6nezdu7fE9r1795KYmHjS906ePJnHH3+cuXPn0rp1a//24vft3buXmjVrljhm27Ztj3sst9uN2+0+w1acniD1AImIiNjO1h4gl8tF+/btmTdvnn+b1+tl3rx5dOnS5YTvmzRpEg8//DCzZ8+mQ4cOJV6rX78+iYmJJY6Znp7OsmXLTnrMQNGtMEREROxnaw8QwOjRoxk6dCgdOnSgU6dOTJkyhaysLIYPHw7AkCFDqFWrFhMnTgTgiSeeYOzYsbz77rskJyf75/VEREQQERGBZVncddddPPLIIzRu3Jj69evz4IMPkpSURP/+/e1qpl+QVoIWERGxne0B6Prrr2f//v2MHTuWlJQU2rZty+zZs/2TmLdv347DcaSjaurUqeTn53PNNdeUOM64ceMYP348AP/85z/JysrilltuITU1lYsuuojZs2eXaZ5QedFCiCIiIvazfR2giuhsrgN09/S1TF+5k3/2bspt3RuV67FFRESqsnNmHaCqyH8VmEe5U0RExC4KQAGmW2GIiIjYTwEowJy6CkxERMR2CkABpgAkIiJiPwWgAAtSABIREbGdAlCAaSFEERER+ykABZgWQhQREbGfAlCAaSFEERER+ykABZizaFVrXQYvIiJiHwWgAHMWfeJaCFFERMQ+CkABph4gERER+ykABZi/B0hzgERERGyjABRg/h4gBSARERHbKAAFmBZCFBERsZ8CUIBpIUQRERH7KQAFmBZCFBERsZ8CUIAdWQjRa3MlIiIiVZcCUID57wavDiARERHbKAAFmD8AqQdIRETENgpAAebUJGgRERHbKQAFmAKQiIiI/RSAAkwBSERExH4KQAGmhRBFRETspwAUYP6FEHUzVBEREdsoAAWYfyFEXQcvIiJiGwWgACteCNGrHiARERHbKAAFmFO3whAREbGdAlCA6SowERER+ykABZgCkIiIiP0UgAJMAUhERMR+CkABpgAkIiJiPwWgAAty+D5yBSARERH7KAAFmLPoE9dCiCIiIvZRAAowZ3EPkBZCFBERsY0CUIAVL4SoHiARERH7KAAFmNOphRBFRETspgAUYP4eIAUgERER2ygABdjRl8EbDYOJiIjYQgEowIoDEIA6gUREROyhABRgRwcgDYOJiIjYQwEowIIUgERERGynABRgJXqANAdIRETEFgpAAVYiAGkxRBEREVsoAAXSuhkEvT+QYc7ZgHqARERE7KIAFEiHtmBt/pLzHNsAKPR6bS5IRESkalIACiRnMAAuPAAo/4iIiNhDASiQHL4AFGz5ApB6gEREROyhABRIziDgSADSZfAiIiL2UAAKpN/1ACkAiYiI2EMBKJB+NwdIAUhERMQeCkCBVNQDFFTcA6TL4EVERGyhABRIxXOAinqACrUQooiIiC0UgAKpeA5Q8WXw6gESERGxhQJQIDlLDoEVag6QiIiILRSAAslRPARWCIBXAUhERMQWCkCBVBSAglAPkIiIiJ0UgAKpeAhMl8GLiIjYSgEokBwKQCIiIhWBAlAgOYuHwHxzgBSARERE7KEAFEhFPUBO47sJqgKQiIiIPRSAAsk/B8jXA6RJ0CIiIvZQAAqkoqvAnFoIUURExFYKQIFU3ANk1AMkIiJiJwWgQCqeA6SFEEVERGylABRIzuIA5AWMeoBERERsogAUSEVzgMB3Q1SP12tjMSIiIlWXAlAgFfUAge9KMI/yj4iIiC0UgALJcSQAqQdIRETEPgpAgVSiB8ijhRBFRERsYnsAevHFF0lOTiYkJITOnTuzfPnyE+67bt06rr76apKTk7EsiylTphyzz/jx47Esq8SjWbNmZ7EFp8GywHICvgCkSdAiIiL2sDUAffDBB4wePZpx48axatUq2rRpQ69evdi3b99x98/OzqZBgwY8/vjjJCYmnvC4LVq0YM+ePf7HokWLzlYTTl9RL1AwhVoIUURExCa2BqCnn36am2++meHDh3Peeecxbdo0wsLCeO211467f8eOHXnyyScZOHAgbrf7hMcNCgoiMTHR/4iPjz9bTTh9xXeEt9QDJCIiYhfbAlB+fj4rV66kZ8+eR4pxOOjZsydLliwp07E3b95MUlISDRo0YPDgwWzfvv2k++fl5ZGenl7icdb47wjv0UKIIiIiNrEtAB04cACPx0NCQkKJ7QkJCaSkpJzxcTt37swbb7zB7NmzmTp1Klu2bKFbt25kZGSc8D0TJ04kOjra/6hTp84Zn/+UHMVDYOoBEhERsYvtk6DLW58+fbj22mtp3bo1vXr1YubMmaSmpvLhhx+e8D333XcfaWlp/seOHTvOXoFH3RFePUAiIiL2CDr1LmdHfHw8TqeTvXv3lti+d+/ek05wPl0xMTE0adKEX3755YT7uN3uk84pKldFq0GrB0hERMQ+tvUAuVwu2rdvz7x58/zbvF4v8+bNo0uXLuV2nszMTH799Vdq1qxZbscsE38PkNYBEhERsYttPUAAo0ePZujQoXTo0IFOnToxZcoUsrKyGD58OABDhgyhVq1aTJw4EfBNnF6/fr3/+127drFmzRoiIiJo1KgRAGPGjKFfv37Uq1eP3bt3M27cOJxOJ4MGDbKnkb931FVgCkAiIiL2sDUAXX/99ezfv5+xY8eSkpJC27ZtmT17tn9i9Pbt23E4jnRS7d69m3bt2vmfT548mcmTJ3PJJZewYMECAHbu3MmgQYM4ePAg1atX56KLLmLp0qVUr149oG07IWfxEFihhsBERERsYmsAAhg1ahSjRo067mvFoaZYcnIy5hSLB77//vvlVdrZ4TgyBKaFEEVEROxR6a4Cq/CcugxeRETEbgpAgXZ0D5ACkIiIiC0UgALNUXwzVM0BEhERsYsCUKAVD4FZ6gESERGxiwJQoB01BKYeIBEREXsoAAXaUTdD1TpAIiIi9lAACjSHVoIWERGxmwJQoDk1BCYiImI3BaBAcxxZB0gLIYqIiNhDASjQ/HOAdBm8iIiIXRSAAu2om6HqMngRERF7KAAFWolbYXhtLkZERKRqUgAKNMeRy+CVf0REROyhABRo/h6gQvUAiYiI2OSMAtCOHTvYuXOn//ny5cu56667ePnll8utsErr6HWANAVIRETEFmcUgP785z8zf/58AFJSUrj00ktZvnw5999/PxMmTCjXAiudEitBqwdIRETEDmcUgH766Sc6deoEwIcffkjLli1ZvHgx77zzDm+88UZ51lf5OI7cDLVQXUAiIiK2OKMAVFBQgNvtBmDu3LlcccUVADRr1ow9e/aUX3WVkX8l6EIthCgiImKTMwpALVq0YNq0aXz77bfMmTOH3r17A7B7926qVatWrgVWOrobvIiIiO3OKAA98cQTvPTSS3Tv3p1BgwbRpk0bAD777DP/0JicQNEcoGC0EKKIiIhdgs7kTd27d+fAgQOkp6cTGxvr337LLbcQFhZWbsVVSuoBEhERsd0Z9QDl5OSQl5fnDz/btm1jypQpbNq0iRo1apRrgZXOUesAqQdIRETEHmcUgK688kreeustAFJTU+ncuTNPPfUU/fv3Z+rUqeVaYKVz1ErQ6gESERGxxxkFoFWrVtGtWzcAPvroIxISEti2bRtvvfUWzz33XLkWWOk4j7oZqq4CExERscUZBaDs7GwiIyMB+OqrrxgwYAAOh4MLLriAbdu2lWuBlY7j6JuhKgCJiIjY4YwCUKNGjfjkk0/YsWMHX375JZdddhkA+/btIyoqqlwLrHSOWgfIo4UQRUREbHFGAWjs2LGMGTOG5ORkOnXqRJcuXQBfb1C7du3KtcBK56g5QB4NgYmIiNjijC6Dv+aaa7jooovYs2ePfw0ggB49enDVVVeVW3GVklNDYCIiInY7owAEkJiYSGJiov+u8LVr19YiiKVx1DpAugxeRETEHmc0BOb1epkwYQLR0dHUq1ePevXqERMTw8MPP4xXdzg/ueIhMEs9QCIiInY5ox6g+++/n3//+988/vjjdO3aFYBFixYxfvx4cnNzefTRR8u1yErFfyuMQgC8XoPDYdlZkYiISJVzRgHozTff5NVXX/XfBR6gdevW1KpVi9tuu00B6GSOGgIDKPQaXApAIiIiAXVGQ2CHDh2iWbNmx2xv1qwZhw4dKnNRldpRk6ABLYYoIiJigzMKQG3atOGFF144ZvsLL7xA69aty1xUpVY0B8h5VA+QiIiIBNYZDYFNmjSJyy+/nLlz5/rXAFqyZAk7duxg5syZ5VpgpeMsOQSmxRBFREQC74x6gC655BJ+/vlnrrrqKlJTU0lNTWXAgAGsW7eOt99+u7xrrFwcJYfAtBiiiIhI4J3xOkBJSUnHTHZeu3Yt//73v3n55ZfLXFilVdQD5LAMDrwUatkAERGRgDujHiApA8eRzBlMIco/IiIigacAFGhFPUDgmwekHiAREZHAUwAKNEfJAKT8IyIiEninNQdowIABJ309NTW1LLVUDQ6n/9tg9QCJiIjY4rQCUHR09ClfHzJkSJkKqvQsy9cL5C0giEIthCgiImKD0wpAr7/++tmqo2pxFgUg3RBVRETEFpoDZIej1gLyKACJiIgEnAKQHYruCB+kACQiImILBSA7+HuACjUEJiIiYgMFIDscdT8wrwKQiIhIwCkA2cFxZAhMPUAiIiKBpwBkB+eRSdDqARIREQk8BSA7FM0BCrI0B0hERMQOCkB2KLoKLBgPHi2EKCIiEnAKQHZwHJkE7fEoAImIiASaApAdjroKTD1AIiIigacAZAdH8RBYoRZCFBERsYECkB2O6gHSJGgREZHAUwCyg/8qMF0GLyIiYgcFIDs4jlwFph4gERGRwFMAsoP/ZqiF6gESERGxgQKQHRxHVoJWD5CIiEjgKQDZQZfBi4iI2EoByA5H3QzV4/HaXIyIiEjVowBkhxI9QDbXIiIiUgUpANnhqMvgPV71AImIiASaApAdnEcmQWsETEREJPAUgOzgOHIZvHqAREREAk8ByA66FYaIiIitFIDscNQ6QFoIUUREJPAUgOzgPHIZvHqAREREAk8ByA7+q8AKtRCiiIiIDWwPQC+++CLJycmEhITQuXNnli9ffsJ9161bx9VXX01ycjKWZTFlypQyH9MWR18FpoWAREREAs7WAPTBBx8wevRoxo0bx6pVq2jTpg29evVi3759x90/OzubBg0a8Pjjj5OYmFgux7TF0StBqwdIREQk4GwNQE8//TQ333wzw4cP57zzzmPatGmEhYXx2muvHXf/jh078uSTTzJw4EDcbne5HNMWJdYBUgASEREJNNsCUH5+PitXrqRnz55HinE46NmzJ0uWLKkwxzwriucAUagAJCIiYoMgu0584MABPB4PCQkJJbYnJCSwcePGgB4zLy+PvLw8//P09PQzOn+pHX0vMAUgERGRgLN9EnRFMHHiRKKjo/2POnXqnN0TFs0BCrZ0GbyIiIgdbAtA8fHxOJ1O9u7dW2L73r17TzjB+Wwd87777iMtLc3/2LFjxxmdv9TUAyQiImIr2wKQy+Wiffv2zJs3z7/N6/Uyb948unTpEtBjut1uoqKiSjzOqqPmAOUWeM7uuUREROQYts0BAhg9ejRDhw6lQ4cOdOrUiSlTppCVlcXw4cMBGDJkCLVq1WLixImAb5Lz+vXr/d/v2rWLNWvWEBERQaNGjUp1zAqhaCXoYDyk5RTYXIyIiEjVY2sAuv7669m/fz9jx44lJSWFtm3bMnv2bP8k5u3bt+NwHOmk2r17N+3atfM/nzx5MpMnT+aSSy5hwYIFpTpmheA4MgSWnqsAJCIiEmiWMVqJ7/fS09OJjo4mLS3t7AyHbVsMr/fhV29NhkdM5Zt//qH8zyEiIlLFnM7vb10FZgf/3eAL1QMkIiJiAwUgOxTfDd7ykJ5TgDrhREREAksByA6OI7fC8BrIzCu0uSAREZGqRQHIDkfdDBUgPVcBSEREJJAUgOxQfDNUqygA6VJ4ERGRgFIAsoPjyDpAoAAkIiISaApAdnAeWQkaNAQmIiISaApAdiiaBO3ECxj1AImIiASYApAdnEcW4A7S7TBEREQCTgHIDkU9QKDbYYiIiNhBAcgOziMBKBgP6TmaAyQiIhJICkB2KNEDpNthiIiIBJoCkB0cDrB8H30QHk2CFhERCTAFILscdTsMTYIWEREJLAUguxSvBWR5tA6QiIhIgCkA2cW/GnShhsBEREQCTAHILv7VoHUZvIiISKApANnFcSQAZeYV4vUamwsSERGpOhSA7OI8ckNUYyBD84BEREQCRgHILkU9QGFBXgANg4mIiASQApBdiuYARbt8T3UpvIiISOAoANmlqAco2m0B6gESEREJJAUguxTNAYos6gHS/cBEREQCRwHILkU9QJHBvqu/tBaQiIhI4CgA2cVZHIB8TzUEJiIiEjgKQHYpWgk6Qj1AIiIiAacAZJeiHqCIoKIApHWAREREAkYByC7F6wCpB0hERCTgFIDsUnQVWLjTF4C0DpCIiEjgKADZpagHKFQrQYuIiAScApBdiiZBhzmLh8A0B0hERCRQFIDsUjQJOtSpHiAREZFAUwCyS1EPUIijKABpDpCIiEjAKADZpagHyF0UgLLyPRR6vHZWJCIiUmUoANmlaBK02/L4N2ktIBERkcBQALJL0WXwDjyEu5yAhsFEREQCRQHILkU9QHgKiQr1fa+J0CIiIoGhAGSXojlAeAuILg5AuhReREQkIBSA7OLvASogKkQ9QCIiIoGkAGSXojlAeAuJCvV9r9thiIiIBIYCkF1cEb6vB3890gOkACQiIhIQCkB2adrXtxjijqU0Mb8BGgITEREJFAUgu0TXgvP6A9Dt0EeAJkGLiIgEigKQnS64DYBm+78injS2H8q2uSAREZGqQQHITrXbQ+1OOE0Bfwmay8Kf97Pw5/12VyUiIlLpKQDZ7YJbAbgp5GtcFHD/jB/JztdQmIiIyNmkAGS35v0gqhYRhYcZErGCnYdzeHbuZrurEhERqdQUgOzmDIZOtwAwJng6kWTz6qItrNudZnNhIiIilZcCUEXQ6RaIa0BIzl6m1fgYj9dw38c/4vEauysTERGplBSAKgJXGFz5ImDRNX0ml4Ws44edaby5eKvdlYmIiFRKCkAVRb0LofPfAHgm5DUiyGbyV5vYlZpjc2EiIiKVjwJQRdJjLMQmE567h+diPyQ738ODn/yEMRoKExERKU8KQBWJKxyu/Bdg8cecrxgQtJivN+7jfz/usbsyERGRSkUBqKJJ7gqX/BOAx12vUd/aw/0zfmKHVokWEREpNwpAFdEl90ByN1zebP4d9iK5OVmMem81+YVeuysTERGpFBSAKiKHEwa8AmHVaOD5jcdD3mTtjsNMnLXB7spEREQqBQWgiiqqJgx4GSwHVzGfO5wzeP27rfzvB80HEhERKSsFoIqsUU/oOxmA0cEfcZ1zPqM/XMOKrYdsLkxEROTcpgBU0XW8EbqNAWBi8L/p6v2eG99YwcaUdJsLExEROXcpAJ0L/vgAtPkzTrxMdT1H87wfGfracl0ZJiIicoYUgM4FlgVXPAdN+uAmn9fdk0nIWM9f/r2MlLRcu6sTERE55ygAnSucwXDtG1D/YsLI4W33E7gPbWLQK0vZl64QJCIicjoUgM4lwSEw8D2o3ZFoMvnA/SjhB39k0CtL2Z+RZ3d1IiIi5wwFoHONOwIGT4ek84klnQ/cj1LtwPdc//IS3ThVRESklBSAzkWhsTD0M0juRnjRcFi9g4u4ZupiftmXaXd1IiIiFZ4C0LnKHenrCSqaGP2K62k6ZnzNtdMWs3ZHqt3ViYiIVGgKQOey4FC4/m1odR1BeJjiepG+ebP48ytL+e6XA3ZXJyIiUmEpAJ3rnMFw1UvQ8WYcGB4Nfo2hno8Z/vpyZv+k22aIiIgcjwJQZeBwQN8n/StG/zP4Ax6wXmPUO9/zzrJtNhcnIiJS8QTZXYCUE8uCHg9CeHXM7HsZEjSHROsQd8wYxea9mTxweXOCnMq7IiIiUEF6gF588UWSk5MJCQmhc+fOLF++/KT7T58+nWbNmhESEkKrVq2YOXNmideHDRuGZVklHr179z6bTag4LhiBdd2bGKeby5wr+cD1MHMWr2D4GytIyy6wuzoREZEKwfYA9MEHHzB69GjGjRvHqlWraNOmDb169WLfvn3H3X/x4sUMGjSIG2+8kdWrV9O/f3/69+/PTz/9VGK/3r17s2fPHv/jvffeC0RzKobzrsQa8imExtLG8Rv/c9+P9es8rnxxkS6TFxERASxjjLGzgM6dO9OxY0deeOEFALxeL3Xq1OH222/n3nvvPWb/66+/nqysLL744gv/tgsuuIC2bdsybdo0wNcDlJqayieffHJGNaWnpxMdHU1aWhpRUVFndIwKIXU7fDgEdq/Gi8WzBQN4Pehanv1ze/7QtIbd1YmIiJSr0/n9bWsPUH5+PitXrqRnz57+bQ6Hg549e7JkyZLjvmfJkiUl9gfo1avXMfsvWLCAGjVq0LRpU2699VYOHjx4wjry8vJIT08v8agUYurCX7+EDn/FgeHvwf/lOe9E/vHG17z8za/YnH1FRERsY2sAOnDgAB6Ph4SEhBLbExISSElJOe57UlJSTrl/7969eeutt5g3bx5PPPEECxcupE+fPng8nuMec+LEiURHR/sfderUKWPLKpAgN/zpGeg/DRMUSnfnWj5z3c8Xs/7Hbe+sIiNX84JERKTqsX0O0NkwcOBArrjiClq1akX//v354osvWLFiBQsWLDju/vfddx9paWn+x44dOwJbcCC0HYR101xMXANqWwf4yDWeOhteof8L3/Lz3gy7qxMREQkoWwNQfHw8TqeTvXv3lti+d+9eEhMTj/uexMTE09ofoEGDBsTHx/PLL78c93W3201UVFSJR6WU2BLrlgVw3pW4LA//F/we49Me5MYXPuf95ds1JCYiIlWGrQHI5XLRvn175s2b59/m9XqZN28eXbp0Oe57unTpUmJ/gDlz5pxwf4CdO3dy8OBBatasWT6Fn8tCouHaN6Hfc5igULo5f+Izx90s++RfjHpnFWk5GhITEZHKz/YhsNGjR/PKK6/w5ptvsmHDBm699VaysrIYPnw4AEOGDOG+++7z73/nnXcye/ZsnnrqKTZu3Mj48eP5/vvvGTVqFACZmZncfffdLF26lK1btzJv3jyuvPJKGjVqRK9evWxpY4VjWdB+KNbfvsEktibWyuQZ11Su2fR3bnj6I774Ybd6g0REpFKzPQBdf/31TJ48mbFjx9K2bVvWrFnD7Nmz/ROdt2/fzp49R+5pdeGFF/Luu+/y8ssv06ZNGz766CM++eQTWrZsCYDT6eSHH37giiuuoEmTJtx44420b9+eb7/9FrfbbUsbK6zqTbBu/hp6jMXrdPMH51o+yL+DXz68n5tfXchv+7VmkIiIVE62rwNUEVWadYBOx/6f8X5+J47tiwHYa2J4yjOQ0A6Dub1nU+IjFB5FRKRiO53f3wpAx1ElAxCAMbDhMwpmP0hwuu8mqj95k5nMEM6/5Apu6lafMJduHyciIhWTAlAZVdkAVKwwD5a9ROGCJwgq8A2DzfW04y3XQHpd1ofrO9TRjVVFRKTCUQAqoyofgIplHcDMn4hZ+ToO41tEcoGnDdPDB9Htj5dz1fm1cAc5bS5SRETERwGojBSAfufAZjwLn8T66SN/EPrO04L/uK+j/cX9GNS5HuFuDY2JiIi9FIDKSAHoBA79RsHCp3H+8B4OUwjA994mfOjoS1KX6xlyUSPiwl02FykiIlWVAlAZKQCdQuoOPN8+A6vfwun1LZy418TwgfdSDjX/M1de1I62dWKwLMvmQkVEpCpRACojBaBSykjBu+I18pe/RkjufgDyjZOZ3s4siunP+V17c2W7WhoeExGRgFAAKiMFoNNUmI9Z/ylZi/5FxL5V/s1bvAl8YV1C3nnX8ocLOnB+3Vj1ComIyFmjAFRGCkBlsHs1eUtewrH+E4I9Of7Ny7zN+Nrdg7C2A+jboSmNEyJtLFJERCojBaAyUgAqB3mZeDd8TvrSt4hKWYID3x+zPBPEIm8rfojsRmy7K7m4bTMaVI+wuVgREakMFIDKSAGonKXtpGD1B+SufIfIjF/9mz3GYrm3Od+HdcVq/ie6tGtN2zqxOB0aJhMRkdOnAFRGCkBniTGwfyM5az8h58dPiEvfWOLln721WOFsR07d7iS26UGXJrWopnuQiYhIKSkAlZECUIAc3kruj5+RtfYTYg+uxoHX/1KeCWaZtxk/R3TEWb8rjVpfSIcGCYS6tPK0iIgcnwJQGSkA2SD7EAW/LODg2pmEbV9IVMG+Ei9nGTdrTGO2R7bFU/sCEs67iPMbJqmHSERE/BSAykgByGbGwIGfyVz3JRkbvyZ6//eEeTJK7FJgnGwyddjiakpOjTaEJHekTpN2NK8VR0iweolERKoiBaAyUgCqYLxezP4NHF6/kKxfviFq7wqiCw8cs1u2cbPeJLMntBH51ZoRVqcN8Q3a0ah2IrG6RYeISKWnAFRGCkAVnDGQtoOsLSs4sGkJ1u5VVM/YQKjJPu7uW70JbHHWIz28PqZaQ0JrNqN6cksa1q1LdFhwgIsXEZGzRQGojBSAzkFeL+bAz6T+uoLUrath7zpiMzYT4zl4wrccMhHscNTiUEhdsiPrY6o1JiKhPnG1GlG7ZhKx4S6tXF2R5WXA1u+g8WXgcNhdjYhUAApAZaQAVIlkHSBn51oO/baWnJSNOA//SlTWNqp59p/0bRkmlBTiOexKJCcsifyI2jhi6+KOr0dU9TpUS6xLjZgIgp36xWubL0bD9/+GP02BDsPtrkbkiJ+/hKhakNjS7kqqnNP5/a27VErlFh5PaNMe1Grao+T2/Cyy9mxi/9Z15KZswjr4C6EZW4nK20OMN5VIK4dIdkDBDkjD99hV8hAHTBSHrVgygquR465OQVgNvBGJOCITcUfXIDS6BlFxNYiplkBMRBgOLfBYvn6Z4/v62wIFIKk4Un6Ed6/zBaC/rwP1IldYCkBSNbnCCa93PuH1zj/2tYIc8g5u4+DOzaSl/Eb+wW0403cSmr2L6Ly9xHgPEYSHeCudeNKhYBsUAJnAvmMPB5Bmwkm3Isl0RpMbHE2+K5ZCdyye0DgIjSUoLIbgsBjcETGERsQQFhVHRFQcEVHROJ26qu0YaTshdbvv+50r7K1F5GhbvvF9Td8FB3+B+Mb21iMnpAAk8nvBobgTm5GU2Iyk473u9eLNOkjagR2k7d1J5oEd5KftwcrcS3D2PkLz9hNakEq4N51Ik4kDQ7SVRTRZ4EkBD5Bb+nIyCSWLMHIdYeQ6wskLiiDfGU5hUAQeVwQmOBwrOAzLHYbTHY7THUawO5ygkHBcoeG4QyMIDgknOCQcV2gErpBwXO4wHBVt+C5jL+xeBU16n/r/mrctOfJ9+i5I2wXRtU7+Hk8hbPsO6nUFp/7pk7Nk2+KS3ysAVVj6V0DkdDkcOCKrExtZndj6x+lBOprXQ37mIdIPpZBxaB/ZqXvJTz9AQeYBrOyDOHMP48o/THBBJi5PJm5PNmEmi3CTTbDlASCCHCLIAe9B8AKF5dOMbOMmDxcFVjAFBFNgBVPocFFoufBYwXgcbjyOYDxON8bhwut0Y5wujNMFQSEQ5IYgN1aQm5DEZrTpPqBsk8Y/+itsWwQDXoXW15583+2LSz7fueLUAWjRMzD/Eeh6F1z60JnXKXIixsD2pUeeb1sM7YfaV0+g/PRf+PZpuOZ1qN7E7mpKTQFI5GxyOHFFVSc+qjrxyafxPmPIy8smI/UQ2RmHycpIpSArlcLsdLy5aXhz0yE3HSs/A/KzsQpzcBTm4CzMwenNJciTS7A3F5fJw21ycZt8QsjDbR1JT2FWHmHkHXVOfL1TZ2ITTNiQxvCBA6kTF3b67z/4qy/8AKx559QBqLgHKKo2pO/0BaAW/U+8vzGw9j3f96vehD/8ny/AFfN6dSWZlN3BXyD7qDXKfh/UK6sFT8CBTbD0X9Bvit3VlJoCkEhFZFm4Q8JxJ4ZDYp1yOaQxhvyCAvJzs8jPyaQgN4uCvBwK83MpzMumMD+PwvwcvAW5eAry8Obn4i3MxRTkYgrzMYV5UJgLnjyswjzw5OPw5JGY+yt18n6hy57/cNkzSfyzd1OGXZh8er1BxeEEYMtC33BYZMLx980+BPs3+L6/YAR89cCp5wHtWw+HfvV9n3MYNnwOra7xPd/wOUwfDn0eh443lb5mqdgK82D9p9CoJ4TFBeac24uCeWJr2PuTb55a2k6Irh2Y89vh4K++8AOw8X9w+VPgODfmLSoAiVQRlmXhcrlwuVwQFVt+Bz6wGfNCRy51riQpbzsPfe4hJNjJoE51S/d+rxfWvu/7PigUCnN8Xepdbjv+/sW/ZOKbQpM+vgC0ew0U5kPQCVb8Xv+Z76vlBOOBVW/5AlB+Nsz8J3gLfF345w/T/KDK4vO7YO270PwKuP7twJyzePir8WVgOWDPGl9v5al6NM9lG/935PusfbBjGdS70L56ToP6fEWkbOIbYzXtC8CUOr5hrIc+X8cv+zJO9q4jti2CtB3gjoY/3u/b9sMHJ9m/aFihXheo1hBCY8GT57v8+ETWf+r72v1ewPL1Mh36DZa8CBm7fa+l74Jf5pauZjl7dq+GGSN8PQtn6qf/+sIPwMYvjlwxeLYV/9ms2+VICKjsw2CbZvq+uiJ9Xzd8bl8tp0kBSETKruudALQ8OIs/NbDILfBy+3tryC0oxaSiNUXDXy36Q5tBvl6aPWtg/8/H37+4B6juhb6rxWp39D0/0TDY/p99Q2aOYOh0MzT8o2/7t0/5JkYD1DjP93XlG6euV84eTwH892bfkOg71/qGK09X6g74/O++74PDwHhhxb/Lt87jyUiBw1sAC+p0PBKAjr5isbLJOuDr8QHo8aDv64bPfXPuzgEKQCJSdnU7Q53OWJ58nqi1mLhwFxv2pPPQ5+vZmJLOnrQccvI9HLPwfF7mkd6Ztn+G8HjfnA2AHz889jx5mb7hLvD1AAHU7uT7unP58WvbUHT8Bpf4eovOH+J7vvo/UJAFtdrDtW/6tm3+0ndJfaBl7of3B8Ose33DcmeTpxBm3QMfDoHctLN7rtO18g04uNn3/aFfYfowX72l5fXAx7dAXhrU6gD9p/q2r3oTCnLKu9qS/PN/WkJItK8XCHzhO/tQ+Z3H64W5D8GMW329mHb6ebYvYCa28v29Cg7z9ebuXm1vXaWkwW4RKR9d74T3/0z4mteYXXMj/9vqIHuVmx9WpVHdSsVNASlWPPuDapLpqo7THU4TttG3IIuD7tq8+3Mc4du3UN/dnT/wJekr3mNm+A1EhAYT7g4i0h1EjQNLqGs8eCNr4YmsTTBA7Q4AmJ0rsMAXYDZ/BY0v9U0+LQ5Y513p+9q0L4TFH7la57JHfZfu1rvINxy3+u2iobIAydwPb/4J9m/0Pf9tPlz7BtRofuy+e9eVHM6Jqefbr7QTzr1e+Oz2I8NDeRnw5w/BGcCbAp/oirucVFgw0fd9p1tg9Tu+Vb6//D/oO+nUx81Ng09H+YacXBFw9Su+zye6LqRthx8/gvNvOPH7jfENo8YmQ8jvbqFQmAdO18k/5+L5P8XBJzzeN0/twCZfOGp2+fHfl3UAvirqPblgBNRsc/J2zn8EFj3t+/6nj+DCO6DdX2DfBl/wCI2BDjdCcMjJj1MaXq/vfJtm+4anf9+GjUXDX00vh+BQ39+59Z/Chs+g1imWCKkAdC+w49C9wETOgNcL0y6CfetO+61PFVzD854BAISSy/fuWwm38vigsDsfeLqz1jTkfGsztwZ9xh+da/jEcyF3FYwi2GkRZrJZHXwTDsvwsvcKbnB8SSh5FOJksbMTF3uW4MHB1eFvkuGIwrIsbsp9k4H5/+XboC48Ev5/WBZ0z1/AvdlPsd+K55a413AGBRPsdFCTvXTL/pqWuStJCWnAqpjepES2wBXkJNjpIDjIQbDTgctp+Z4Xb7Mgp9BLWk4BGbmFBDktokKCiQwJwgIKPAZnzgH+tPoWqmX/RkZwdcAQWXCAfMvN4sQbOJhwIQUJbUlMW0PTzS9T8+DSYz679PBkDtbtTUF0fYKzduPK2k2hM5QD8Rewt1onCoLCKPQYPB4vbdY/QdOt/8FrOfFawQR5c9lU+xqWNLufXI8hr8CLxxhcTotwsgkzuXjCE3EFOwl2WkRkbqXOjs9w5x1iT91+pFbvgOVwEFSQSdL2zwnP3klGk6tw1GyNK8hBVl4hmbmFZOV7cKX+RtMfHid+/1J2JQ9gZ4tb8UbWxOM1eI2h4ZpJ1Nv4KjnRDdk04Csits6h0fwRAOxsOJCNLUaT7YzE5KTTdNOL1NnzFenRzdif/CeIqkPjJWMIzdyB1xHMhi5PkdbgcoIcDuLXTqXBmkmkRjVj3sUf4XQ6sCxwOiwclu8RlbaRZmseI27/Mgpc0WxvfjO7mw4hLGsntTe8RvWtn5Ed24wtnSeQU70NhV4vhR5DoddLgcdQ6DF0nXcVMWkbWHvBMxyq/ycsoPHyB6j16wdsbfJXNrW5B2MMXgPeoq/V9i3h/O/vITTvyL0JU6pdwNaafclzxZAXHElORD2sqJqEBjups+2/NFt2n+/nHtuSqMM/HffvU1p0c1Z0mkJWeB1M0fn8X/FdEWoMOAsyCc3ZTU5wHLnBMRjLgdfr2wdPAd3WjaXR3ln+4+5qcB0pXcaBKwyrMIe2756Pw5PL+iu+IDuuBbG/fU7Db+4gJzKZRb2/JCvfQ16hh/AgqL9/LvGHVpNW+w8crnkxxrJIig6lbrUzWDbjJHQz1DJSABI5Q9mHfBNBM/b4JhXnZ0NEDUxEDfJMEHkHt+E9uBUyU/Dm52AKcsh2RDAjeRwphWFk5nmwgCv3v0SPQ0cujS8giOCjVoD8R/4I/uu92P98tusemjl2+J/vMtWoZR30P1/kacFfCu73P3dRQB/HMuZ4O5CN7/+U3eSzxD2KOCuTzz0XAFDH2kdbx7HDDL96a/KbqUk+QRQSRBh5RFrZRJFNlJVFFNlEkMM+YtjorctGU4d8gogkh0grp2hxy2waOXaTaB0mxcQyMP8BMkwYzwT/i4udRyZ0Fxinf1HMAuNknamHwYEDL82sHbitghP+OAqMk19MEgdNFF4c/uOOzh9BOuG8HPw0DsvwemEvdptqxFkZ1LX20sLaRrJjLwCpJpyNpi4h5NPWUXJi8k/eZH7w1qefcymR1pEhpq89bXnLcymZJpQgy8sljrXc6JyJyzoyJyzPBDPdczGbTW3yCOahoDdwW4X8NX8MX3t9vQcjnJ9xb7DvCsH9Jor3PH9kkHM+1a3jD93tNPGMzL+DtaaRf1sMGSx1jyLEKmBqYT+CKaSGlUoBTrJNCFFWNn9yLMFplfxVmG5CibJKDpt5jcU7nh5sMnVo6/iVFtYWLCCNcDpYm3Bahs65L7AX32X3/R2LmOL6FxkmlF9NTbJNCFmEkIMbB176OpbjsAybvbXYYOrS17GMIMt7TLvWeBuyzNuMvzpnE2x5eK6wP08XXksvx/fcH/QfalkH+NnUZr1JprtjDdWsDNJNKE8XXsshE0kQHrw4yMFNNm6SrIP0diynq+Mn/8+kwDjZY+JYYZqyzNucPzmWcrHzRwqMky+9Hfy1bvdW5ytvB/IJ5ragz9hp4rko71nAIoJsVrpH4LYKeargGnaYGsRaGQx3zqau40jIW+ttwHOFV9G027X8s89xejrLQAGojBSARGzm9cKWBbD2A9+kyoIs3/ydxpdBsz+R37gvWflecgo8OB0WUfPvJ3T1qxSG12TfBfexp04/Qg/8SMK6V4lMWcJvXSeTltSt6P9+wWAo+g+O2lZ/5WPU3vhaiVIMDvbGd2Zb9e5UO/wD9fbNI9h7GvcyOYW0oHhea/wCGWH1CA6ycDmg9f7/Uefgt9ROX0OEJ5V8y8WiyL7Mj7uejJCaBDsdBDkdkJdO8sFvaZn+DaHeLPY7a3DQWZ1q3kO0KVhDomfPMef7MH4kC+OuwQK6H/6Ia/a/eOIfQ1HQKubBwVp3ew474rgoZwHuoxbS3OWsxTZHHToXLMfJsb/EAda4O7AwvBeXZnzKeQXH9l6sDmrNP9wTyPeaot40i3aenxiVM5U6niMBNyWoFp/HDSM+dyuds+aT5NnN4uALeC7iLnKCoij0eMkv9FLoNbiDHIzJfYGeuV+e9Ofwnfsi3gwfTrP89QzMfpck7x48OFgcfAGzgnvxx4Kv6Vmw8KTH2OWsw6hqL5Ff6Gt/nDnMq4dvKvE5/d788D68E3srhc5QqhfupUfmp9TO30q4N4NwTzo1CneX2H+h6xKejrzbtyK9BZYxBFsevI5gLCyqefdzZ+rjNMsvXU9sliOCUG8WDo6NAnlWCK/XeoifwjpR6/Bybjr4JNW9B0rs83FQX55z3wL4ltp4LOcRuniOvSAhzYpiubMdFxUuJbTo89iceDmNR7xbqjpLSwGojBSARCqQ/Cw4vA3im5x4jZ7cdN+l7Q3+AO6IMz9XTqrvyjCH0zdPKKKG795hUTWP7JOXAb9+7dvXk+97BIf55o2ERENIjO+rK9x3RdK+dbBvI2DAHXnUI9r3tV4X3/7HY4zvyqKQmDNbzO/w1qLViQ/55prUaHbkKrji4y96BrZ+C2HVILw6RNb0TWpNbO37LPdv9M09KszzzQGJqOF7b/Yh3+Tiw1t986vqd/fN7Tn4Kyx+zndTUMsBjiBfeO1655H7vBnjm+u0/jPflV65ab7Pp+9TEN/omGZQmA9LnocfPoTW10OXkUdW8jYGMvdCRMKJ5+ik7fRN/HYG++7SHpXkmzCdn+VbQqFxryOT6sF3Ndr2pb45ZHH1j2zf8o3v6kFHECSdD0ntfHNfclN9bUju5lua4Wjpu32fSX4W5Gf6vhZk+75POh8a9Tj5zzAjBTbN8k04DomGfs+den6PpwC+mwK/LSz6GTh9k5Xzs33nDg6DJr2geT+o3tS3f+Y+33ylrd/57pmXcxiufNE/xw7w/T3bNMt3wcHOFb59/jzd9+eq2KHfYPkrvp9J1gHf34+WV0PbweAK821b8oJvn/5T4bwrTt6W06QAVEYKQCIiImdR9iFfsC/nW9Cczu9vXQUmIiIigRWo25OchNYBEhERkSpHAUhERESqHAUgERERqXIUgERERKTKUQASERGRKkcBSERERKocBSARERGpchSAREREpMpRABIREZEqRwFIREREqhwFIBEREalyFIBERESkylEAEhERkSpHd4M/DmMMAOnp6TZXIiIiIqVV/Hu7+Pf4ySgAHUdGRgYAderUsbkSEREROV0ZGRlER0efdB/LlCYmVTFer5fdu3cTGRmJZVnleuz09HTq1KnDjh07iIqKKtdjV0RVrb1Q9dpc1doLVa/NVa29UPXaXFnaa4whIyODpKQkHI6Tz/JRD9BxOBwOateufVbPERUVdU7/ITtdVa29UPXaXNXaC1WvzVWtvVD12lwZ2nuqnp9imgQtIiIiVY4CkIiIiFQ5CkAB5na7GTduHG632+5SAqKqtReqXpurWnuh6rW5qrUXql6bq1p7QZOgRUREpApSD5CIiIhUOQpAIiIiUuUoAImIiEiVowAkIiIiVY4CUAC9+OKLJCcnExISQufOnVm+fLndJZWLiRMn0rFjRyIjI6lRowb9+/dn06ZNJfbJzc1l5MiRVKtWjYiICK6++mr27t1rU8Xl7/HHH8eyLO666y7/tsrW5l27dvGXv/yFatWqERoaSqtWrfj+++/9rxtjGDt2LDVr1iQ0NJSePXuyefNmGysuG4/Hw4MPPkj9+vUJDQ2lYcOGPPzwwyXuMXSut/mbb76hX79+JCUlYVkWn3zySYnXS9O+Q4cOMXjwYKKiooiJieHGG28kMzMzgK0ovZO1t6CggHvuuYdWrVoRHh5OUlISQ4YMYffu3SWOcS61F079Mz7aiBEjsCyLKVOmlNh+rrW5tBSAAuSDDz5g9OjRjBs3jlWrVtGmTRt69erFvn377C6tzBYuXMjIkSNZunQpc+bMoaCggMsuu4ysrCz/Pn//+9/5/PPPmT59OgsXLmT37t0MGDDAxqrLz4oVK3jppZdo3bp1ie2Vqc2HDx+ma9euBAcHM2vWLNavX89TTz1FbGysf59Jkybx3HPPMW3aNJYtW0Z4eDi9evUiNzfXxsrP3BNPPMHUqVN54YUX2LBhA0888QSTJk3i+eef9+9zrrc5KyuLNm3a8OKLLx739dK0b/Dgwaxbt445c+bwxRdf8M0333DLLbcEqgmn5WTtzc7OZtWqVTz44IOsWrWKjz/+mE2bNnHFFVeU2O9cai+c+mdcbMaMGSxdupSkpKRjXjvX2lxqRgKiU6dOZuTIkf7nHo/HJCUlmYkTJ9pY1dmxb98+A5iFCxcaY4xJTU01wcHBZvr06f59NmzYYACzZMkSu8osFxkZGaZx48Zmzpw55pJLLjF33nmnMabytfmee+4xF1100Qlf93q9JjEx0Tz55JP+bampqcbtdpv33nsvECWWu8svv9z89a9/LbFtwIABZvDgwcaYytdmwMyYMcP/vDTtW79+vQHMihUr/PvMmjXLWJZldu3aFbDaz8Tv23s8y5cvN4DZtm2bMebcbq8xJ27zzp07Ta1atcxPP/1k6tWrZ5555hn/a+d6m09GPUABkJ+fz8qVK+nZs6d/m8PhoGfPnixZssTGys6OtLQ0AOLi4gBYuXIlBQUFJdrfrFkz6tate863f+TIkVx++eUl2gaVr82fffYZHTp04Nprr6VGjRq0a9eOV155xf/6li1bSElJKdHe6OhoOnfufE62F+DCCy9k3rx5/PzzzwCsXbuWRYsW0adPH6BytvlopWnfkiVLiImJoUOHDv59evbsicPhYNmyZQGvubylpaVhWRYxMTFA5Wyv1+vlhhtu4O6776ZFixbHvF4Z21xMN0MNgAMHDuDxeEhISCixPSEhgY0bN9pU1dnh9Xq566676Nq1Ky1btgQgJSUFl8vl/0ekWEJCAikpKTZUWT7ef/99Vq1axYoVK455rbK1+bfffmPq1KmMHj2a//u//2PFihXccccduFwuhg4d6m/T8f6Mn4vtBbj33ntJT0+nWbNmOJ1OPB4Pjz76KIMHDwaolG0+Wmnal5KSQo0aNUq8HhQURFxc3Dn/GeTm5nLPPfcwaNAg/81BK2N7n3jiCYKCgrjjjjuO+3plbHMxBSApVyNHjuSnn35i0aJFdpdyVu3YsYM777yTOXPmEBISYnc5Z53X66VDhw489thjALRr146ffvqJadOmMXToUJurOzs+/PBD3nnnHd59911atGjBmjVruOuuu0hKSqq0bRafgoICrrvuOowxTJ061e5yzpqVK1fy7LPPsmrVKizLsrucgNMQWADEx8fjdDqPuQJo7969JCYm2lRV+Rs1ahRffPEF8+fPp3bt2v7tiYmJ5Ofnk5qaWmL/c7n9K1euZN++fZx//vkEBQURFBTEwoULee655wgKCiIhIaFStblmzZqcd955JbY1b96c7du3A/jbVJn+jN99993ce++9DBw4kFatWnHDDTfw97//nYkTJwKVs81HK037EhMTj7mQo7CwkEOHDp2zn0Fx+Nm2bRtz5szx9/5A5Wvvt99+y759+6hbt67/37Ft27bxj3/8g+TkZKDytfloCkAB4HK5aN++PfPmzfNv83q9zJs3jy5duthYWfkwxjBq1ChmzJjB119/Tf369Uu83r59e4KDg0u0f9OmTWzfvv2cbX+PHj348ccfWbNmjf/RoUMHBg8e7P++MrW5a9euxyxt8PPPP1OvXj0A6tevT2JiYon2pqens2zZsnOyveC7KsjhKPlPpNPpxOv1ApWzzUcrTfu6dOlCamoqK1eu9O/z9ddf4/V66dy5c8BrLqvi8LN582bmzp1LtWrVSrxe2dp7ww038MMPP5T4dywpKYm7776bL7/8Eqh8bS7B7lnYVcX7779v3G63eeONN8z69evNLbfcYmJiYkxKSordpZXZrbfeaqKjo82CBQvMnj17/I/s7Gz/PiNGjDB169Y1X3/9tfn+++9Nly5dTJcuXWysuvwdfRWYMZWrzcuXLzdBQUHm0UcfNZs3bzbvvPOOCQsLM//5z3/8+zz++OMmJibGfPrpp+aHH34wV155palfv77JycmxsfIzN3ToUFOrVi3zxRdfmC1btpiPP/7YxMfHm3/+85/+fc71NmdkZJjVq1eb1atXG8A8/fTTZvXq1f6rnkrTvt69e5t27dqZZcuWmUWLFpnGjRubQYMG2dWkkzpZe/Pz880VV1xhateubdasWVPi37K8vDz/Mc6l9hpz6p/x7/3+KjBjzr02l5YCUAA9//zzpm7dusblcplOnTqZpUuX2l1SuQCO+3j99df9++Tk5JjbbrvNxMbGmrCwMHPVVVeZPXv22Ff0WfD7AFTZ2vz555+bli1bGrfbbZo1a2ZefvnlEq97vV7z4IMPmoSEBON2u02PHj3Mpk2bbKq27NLT082dd95p6tata0JCQkyDBg3M/fffX+KX4bne5vnz5x/37+7QoUONMaVr38GDB82gQYNMRESEiYqKMsOHDzcZGRk2tObUTtbeLVu2nPDfsvnz5/uPcS6115hT/4x/73gB6Fxrc2lZxhy1rKmIiIhIFaA5QCIiIlLlKACJiIhIlaMAJCIiIlWOApCIiIhUOQpAIiIiUuUoAImIiEiVowAkIiIiVY4CkIjICViWxSeffGJ3GSJyFigAiUiFNGzYMCzLOubRu3dvu0sTkUogyO4CREROpHfv3rz++usltrndbpuqEZHKRD1AIlJhud1uEhMTSzxiY2MB3/DU1KlT6dOnD6GhoTRo0ICPPvqoxPt//PFH/vjHPxIaGkq1atW45ZZbyMzMLLHPa6+9RosWLXC73dSsWZNRo0aVeP3AgQNcddVVhIWF0bhxYz777DP/a4cPH2bw4MFUr16d0NBQGjdufExgE5GKSQFIRM5ZDz74IFdffTVr165l8ODBDBw4kA0bNgCQlZVFr169iI2NZcWKFUyfPp25c+eWCDhTp05l5MiR3HLLLfz444989tlnNGrUqMQ5HnroIa677jp++OEH+vbty+DBgzl06JD//OvXr2fWrFls2LCBqVOnEh8fH7gPQETOnN13YxUROZ6hQ4cap9NpwsPDSzweffRRY4wxgBkxYkSJ93Tu3NnceuutxhhjXn75ZRMbG2syMzP9r//vf/8zDofDpKSkGGOMSUpKMvfff/8JawDMAw884H+emZlpADNr1ixjjDH9+vUzw4cPL58Gi0hAaQ6QiFRYf/jDH5g6dWqJbXFxcf7vu3TpUuK1Ll26sGbNGgA2bNhAmzZtCA8P97/etWtXvF4vmzZtwrIsdu/eTY8ePU5aQ+vWrf3fh4eHExUVxb59+wC49dZbufrqq1m1ahWXXXYZ/fv358ILLzyjtopIYCkAiUiFFR4efsyQVHkJDQ0t1X7BwcElnluWhdfrBaBPnz5s27aNmTNnMmfOHHr06MHIkSOZPHlyudcrIuVLc4BE5Jy1dOnSY543b94cgObNm7N27VqysrL8r3/33Xc4HA6aNm1KZGQkycnJzJs3r0w1VK9enaFDh/Kf//yHKVOm8PLLL5fpeCISGOoBEpEKKy8vj5SUlBLbgoKC/BONp0+fTocOHbjooot45513WL58Of/+978BGDx4MOPGjWPo0KGMHz+e/fv3c/vtt3PDDTeQkJAAwPjx4xkxYgQ1atSgT58+ZGRk8N1333H77beXqr6xY8fSvn17WrRoQV5eHl988YU/gIlIxaYAJCIV1uzZs6lZs2aJbU2bNmXjxo2A7wqt999/n9tuu42aNWvy3nvvcd555wEQFhbGl19+yZ133knHjh0JCwvj6quv5umnn/Yfa+jQoeTm5vLMM88wZswY4uPjueaaa0pdn8vl4r777mPr1q2EhobSrVs33n///XJouYicbZYxxthdhIjI6bIsixkzZtC/f3+7SxGRc5DmAImIiEiVowAkIiIiVY7mAInIOUmj9yJSFuoBEhERkSpHAUhERESqHAUgERERqXIUgERERKTKUQASERGRKkcBSERERKocBSARERGpchSAREREpMpRABIREZEq5/8BH14T5OKWD3MAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# \\fit model with 150 epochs and use batch size of 32\n",
    "\n",
    "history = model.fit(train_data, train_label, epochs = 150, batch_size = 32, verbose = 1, validation_data = (test_data, test_label))\n",
    "\n",
    "# TODO: implement model evaluation and print/graph anything needed for visualization\n",
    "\n",
    "# graph loss\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='test_loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2348/2348 [==============================] - 2s 679us/step\n",
      "number active = 15772\n",
      "number inactive = 59356\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# save and example of how to use model for prediction\n",
    "model.model.save(\"../elderly_activity_model.h5\")\n",
    "\n",
    "loaded_model = load_model(\"../elderly_activity_model.h5\")\n",
    "\n",
    "input_data = pd.DataFrame({\n",
    "            'time': [544.00],\n",
    "            'room': [2],\n",
    "            'frontal accel': [0.16650],\n",
    "            'vertical accel': [1.0798],\n",
    "            'lateral accel': [0.100350],\n",
    "            'antenna id': [1],\n",
    "            'rssi': [-0.597015],\n",
    "            'phase': [0.15493],\n",
    "            'frequency': [922.75],\n",
    "            'gender': [1],\n",
    "            'consecutiveness': [1],\n",
    "            'activity': [4],\n",
    "            'acceleration': [0.038066],\n",
    "            'freq': [0.030495],\n",
    "                }).values\n",
    "#input_data = X\n",
    "predictions = loaded_model.predict(input_data)\n",
    "\n",
    "# apply threshold on our prediction probabilities\n",
    "threshold = 0.043\n",
    "predictions = (predictions >= threshold).astype(int)\n",
    "print(f'number active = {(predictions == 1).sum()}')\n",
    "print(f'number inactive = {(predictions == 0).sum()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
